<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Henry Li">





<title>PyTorch学习笔记 | Henry&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const pagebody = document.getElementsByTagName('body')[0]

            function setTheme(status) {

                if (status === 'dark') {
                    window.sessionStorage.theme = 'dark'
                    pagebody.classList.add('dark-theme');

                } else if (status === 'light') {
                    window.sessionStorage.theme = 'light'
                    pagebody.classList.remove('dark-theme');
                }
            };

            setTheme(window.sessionStorage.theme)
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Henry&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                <a class="menu-item" href="/archives">文章</a>
                
                <a class="menu-item" href="/category">分类</a>
                
                <a class="menu-item" href="/tag">标签</a>
                
                <a class="menu-item" href="/about">关于</a>
                
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Henry&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">
                    <svg class="menu-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M4.5 17.27q-.213 0-.356-.145T4 16.768t.144-.356t.356-.143h15q.213 0 .356.144q.144.144.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.144T4 11.999t.144-.356t.356-.143h15q.213 0 .356.144t.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.143Q4 7.443 4 7.23t.144-.356t.356-.143h15q.213 0 .356.144T20 7.23t-.144.356t-.356.144z"/></svg>
                    <svg class="close-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Material Symbols Light by Google - https://github.com/google/material-design-icons/blob/master/LICENSE --><path fill="currentColor" d="m12 12.708l-5.246 5.246q-.14.14-.344.15t-.364-.15t-.16-.354t.16-.354L11.292 12L6.046 6.754q-.14-.14-.15-.344t.15-.364t.354-.16t.354.16L12 11.292l5.246-5.246q.14-.14.345-.15q.203-.01.363.15t.16.354t-.16.354L12.708 12l5.246 5.246q.14.14.15.345q.01.203-.15.363t-.354.16t-.354-.16z"/></svg>
                </div>
            </div>
            <div class="menu" id="mobile-menu">
                
                <a class="menu-item" href="/archives">文章</a>
                
                <a class="menu-item" href="/category">分类</a>
                
                <a class="menu-item" href="/tag">标签</a>
                
                <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.classList.contains("active")) {
            toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        } else {
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">关闭目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">前往底部</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 6
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        // b.innerText = expanded ? 'Expand all' : 'Collapse all';
        b.innerText = expanded ? '展开目录' : '关闭目录';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">PyTorch学习笔记</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Henry Li</a>&nbsp;&nbsp;
                    

                    
                        <span class="post-time">
                        Date: <a href="#">2025-08-25</a>&nbsp;&nbsp;
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E6%8A%80%E6%9C%AF%E9%80%9F%E8%AE%B0/">技术速记</a>
                            &nbsp;&nbsp;
                        </span>
                    
                    <!-- 添加本文阅读量 -->
                        <span id="busuanzi_container_page_pv">
                        本文阅读量: <span id="busuanzi_value_page_pv"></span> 次
                        </span>

                </div>
            
        </header>

        <div class="post-content">
            <p>本笔记记录刚学习PyTorch时的使用方法，</p>
<p>包括B站视频博主1、<strong>小土堆</strong>、1、书籍<strong>Pytorch深度学习实践</strong>、3、<strong>现有网络结构</strong>等</p>
<h1 id="一、小土堆"><a href="#一、小土堆" class="headerlink" title="一、小土堆"></a>一、小土堆</h1><h2 id="1-配置环境"><a href="#1-配置环境" class="headerlink" title="1. 配置环境"></a>1. 配置环境</h2><h3 id="1-1-环境管理"><a href="#1-1-环境管理" class="headerlink" title="1.1 环境管理"></a>1.1 环境管理</h3><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#在Anaconda Prompt里管理环境</span><br><span class="line">#初始环境为:(base) C:\Users\User&gt;</span><br><span class="line"></span><br><span class="line">#查看已有环境</span><br><span class="line">$conda env list</span><br><span class="line"></span><br><span class="line">#创建环境</span><br><span class="line">$conda create -n &lt;envname&gt; &lt;pkgname=X.x&gt;</span><br><span class="line">$conda create -n mypytorch python=<span class="number">3</span>.<span class="number">11</span></span><br><span class="line"></span><br><span class="line">#移除环境</span><br><span class="line">$conda remove --name &lt;envname&gt; --all</span><br><span class="line"></span><br><span class="line">#激活环境</span><br><span class="line">$conda activate &lt;envname&gt;</span><br><span class="line"></span><br><span class="line">#查看已激活的环境</span><br><span class="line">$conda deactivate</span><br></pre></td></tr></table></figure>

<h3 id="1-2-安装pytorch"><a href="#1-2-安装pytorch" class="headerlink" title="1.2 安装pytorch"></a>1.2 安装pytorch</h3><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#<span class="number">1</span>. 列出当前环境有哪些工具包</span><br><span class="line">$pip list</span><br><span class="line"></span><br><span class="line">#<span class="number">2</span>. 检查CUDA环境</span><br><span class="line">$nvidia-smi</span><br><span class="line">#CUDA Version: <span class="number">12</span>.<span class="number">2</span>，指最高支持CUDA版本，想要更高版本需要更新驱动</span><br><span class="line"></span><br><span class="line">#<span class="number">3</span>. 选择安装pytorch包</span><br><span class="line">#https://pytorch.org/官网选择环境</span><br><span class="line"></span><br><span class="line">#<span class="number">4</span>. 验证是否安装成功</span><br><span class="line">$python	#进入Python环境</span><br><span class="line">&gt;&gt;&gt;import torch</span><br><span class="line">&gt;&gt;&gt;torch.cuda.is_available()	#返回True可用</span><br><span class="line"></span><br><span class="line">#<span class="number">5</span>. 其他可能用到的包</span><br><span class="line">$pip install opencv-python</span><br><span class="line">$conda install tensorboard</span><br><span class="line">$conda install numpy</span><br></pre></td></tr></table></figure>

<h3 id="1-3-创建工程"><a href="#1-3-创建工程" class="headerlink" title="1.3 创建工程"></a>1.3 创建工程</h3><h5 id="1-PyCharm"><a href="#1-PyCharm" class="headerlink" title="1.PyCharm"></a>1.PyCharm</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">选择编译环境</span><br><span class="line">Previously configured interpreter</span><br><span class="line">Add Interpreter</span><br><span class="line">Conda Environment</span><br><span class="line">Use existing environment</span><br></pre></td></tr></table></figure>

<h5 id="2-JupyterNotebook"><a href="#2-JupyterNotebook" class="headerlink" title="2.JupyterNotebook"></a>2.JupyterNotebook</h5><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#在环境中安装jupyter</span><br><span class="line">$conda install jupyter</span><br><span class="line"></span><br><span class="line">#启动jupyter</span><br><span class="line">$jupyter notebook</span><br></pre></td></tr></table></figure>

<h3 id="1-4-Linux运行命令"><a href="#1-4-Linux运行命令" class="headerlink" title="1.4 Linux运行命令"></a>1.4 Linux运行命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#nohup后台运行，控制台输出重定向到log_name.log文件</span></span><br><span class="line"><span class="variable">$nohup</span> python [code_file].py &gt; [log_name].<span class="built_in">log</span> 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="variable">$nohup</span> mim train mmpretrain convnext-b.py --work-dir=./convnext-b-pubds-bs32-328 &gt; convnext-b-pubds-bs32-328.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h2 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2. 数据处理"></a>2. 数据处理</h2><h3 id="2-1-读取数据"><a href="#2-1-读取数据" class="headerlink" title="2.1 读取数据"></a>2.1 读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,root_dir,label_dir</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root_dir=root_dir</span><br><span class="line">        <span class="variable language_">self</span>.label_dir=label_dir</span><br><span class="line">        <span class="variable language_">self</span>.path=os.path.join(<span class="variable language_">self</span>.root_dir,<span class="variable language_">self</span>.label_dir)</span><br><span class="line">        <span class="variable language_">self</span>.img_path=os.listdir(<span class="variable language_">self</span>.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        img_name=<span class="variable language_">self</span>.img_path[item]</span><br><span class="line">        img_item_path=os.path.join(<span class="variable language_">self</span>.root_dir,<span class="variable language_">self</span>.label_dir,img_name)</span><br><span class="line">        img=Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label=<span class="variable language_">self</span>.label_dir</span><br><span class="line">        <span class="keyword">return</span> img,label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root_dir=<span class="string">&quot;dataset/hymenoptera_data/train&quot;</span></span><br><span class="line">ants_label_dir=<span class="string">&quot;ants&quot;</span></span><br><span class="line">bees_label_dir=<span class="string">&quot;bees&quot;</span></span><br><span class="line">ants_dataset=MyData(root_dir,ants_label_dir)</span><br><span class="line">bees_dataset=MyData(root_dir,bees_label_dir)</span><br><span class="line">train_dataset=ants_dataset+bees_dataset</span><br><span class="line"></span><br><span class="line">img,label=ants_dataset[<span class="number">0</span>]</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>

<h3 id="2-2-TensorBoard"><a href="#2-2-TensorBoard" class="headerlink" title="2.2 TensorBoard"></a>2.2 TensorBoard</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#writer=SummaryWriter(&quot;logs&quot;)</span></span><br><span class="line"><span class="comment"># 两个方法,写数据、写图片</span></span><br><span class="line"><span class="comment"># writer.add_image()</span></span><br><span class="line"><span class="comment"># writer.add_scalar()</span></span><br><span class="line"><span class="comment">#需要writer.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开tensorboard,命令行输入</span></span><br><span class="line"><span class="comment">#tensorboard - -logdir = logs</span></span><br><span class="line"><span class="comment">#tensorboard --logdir=logs --port=6007</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一:写数据</span></span><br><span class="line">add_scalar(<span class="variable language_">self</span>,tag,scalar_value,global_step=<span class="literal">None</span>,walltime=<span class="literal">None</span>,new_style=<span class="literal">False</span>,double_precision=<span class="literal">False</span>,)</span><br><span class="line"><span class="comment"># tag (str): Data identifier，标识标量值的标签，通常标识为Loss或train</span></span><br><span class="line"><span class="comment"># scalar_value (float or string/blobname): Value to save，想要记录的标量值，通常为损失值或准确率</span></span><br><span class="line"><span class="comment"># global_step (int): Global step value to record，全局步长值，通常为迭代次数或批次编号，用于显示进度</span></span><br><span class="line"><span class="comment"># walltime (float): Optional override default walltime (time.time()) with seconds after epoch of event，时间戳(单位秒)，对于在图表中按实际时间来排序事件很有用</span></span><br><span class="line"><span class="comment"># new_style (boolean): Whether to use new style (tensor field) or old style (simple_value field). New style could lead to faster data loading.使用新的TensorBoard标量记录器风格</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">writer=SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="comment">#y=2x</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=2x&#x27;</span>, i * <span class="number">2</span>, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法二:写图片</span></span><br><span class="line">add_image(<span class="variable language_">self</span>, tag, img_tensor, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>, dataformats=<span class="string">&quot;CHW&quot;</span>):</span><br><span class="line"><span class="comment"># tag (str): Data identifier，通常标识为Images或input</span></span><br><span class="line"><span class="comment"># img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data，记录的数据，应该是3维HWC或4维NCHW</span></span><br><span class="line"><span class="comment"># global_step (int): Global step value to record，全局步长值，通常为迭代次数或批次编号，用于显示进度</span></span><br><span class="line"><span class="comment"># walltime (float): Optional override default walltime (time.time()) seconds after epoch of event，时间戳(单位秒)，对于在图表中按实际时间来排序事件很有用</span></span><br><span class="line"><span class="comment"># dataformats (str): Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.输入图像数据的格式(默认值是&quot;CHW&quot;),如果提供的图像数据格式与默认不同，则需要指定该参数</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">writer=SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">image_path=<span class="string">&quot;dataset/hymenoptera_data/train/ants/0013035.jpg&quot;</span></span><br><span class="line">img_PIL=Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">img_array=np.array(img_PIL)</span><br><span class="line"><span class="comment"># print(type(img_array))</span></span><br><span class="line"><span class="comment"># print(img_array.shape)</span></span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>,img_array,<span class="number">1</span>,dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>,img_array,<span class="number">2</span>,dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h3 id="2-3-Transforms"><a href="#2-3-Transforms" class="headerlink" title="2.3 Transforms"></a>2.3 Transforms</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于对图片进行变换</span></span><br><span class="line"><span class="comment"># tensor数据类型</span></span><br><span class="line"><span class="comment"># 通过transforms.ToTensor解决两个问题</span></span><br><span class="line"><span class="comment"># 1.transforms如何使用</span></span><br><span class="line"><span class="comment"># 2.为什么需要tensor数据类型</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">img_path=<span class="string">&quot;dataset/hymenoptera_data/train/ants/0013035.jpg&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转为tensor类型的图片</span></span><br><span class="line"><span class="comment"># transforms.ToTensor() #(PIL Image or numpy.ndarray): Image to be converted to tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># img_cv=cv2.imread(img_path)</span></span><br><span class="line">img_PIL=Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">trans_totensor=transforms.ToTensor()</span><br><span class="line">img_tensor=trans_totensor(img_PIL)</span><br><span class="line"><span class="comment"># print(tensor_img)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize标准化</span></span><br><span class="line"><span class="comment"># Normalize a tensor image with mean and standard deviation.</span></span><br><span class="line"><span class="comment"># output[channel] = (input[channel] - mean[channel]) / std[channel]</span></span><br><span class="line"><span class="comment"># transforms.Normalize(mean, std, inplace=False)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img_tensor[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">trans_norm=transforms.Normalize([<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>],[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>])</span><br><span class="line">img_norm=trans_norm(img_tensor)</span><br><span class="line"><span class="built_in">print</span>(img_tensor[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Resize</span></span><br><span class="line"><span class="comment"># Resize the input image to the given size.</span></span><br><span class="line"><span class="comment"># If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img_tensor.size())</span><br><span class="line">trans_resize=transforms.Resize((<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line"><span class="comment"># img_PIL -&gt; resize() -&gt; img_resize_PIL</span></span><br><span class="line">img_resize=trans_resize(img_PIL)</span><br><span class="line"><span class="comment"># img_resize_PIL -&gt; totensor.() -&gt; img_resize_tensor</span></span><br><span class="line">img_resize=trans_totensor(img_resize)</span><br><span class="line"><span class="built_in">print</span>(img_resize)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compose</span></span><br><span class="line"><span class="comment"># 对resize和totensor进行整合</span></span><br><span class="line"></span><br><span class="line">trans_resize_2=transforms.Resize(<span class="number">512</span>)</span><br><span class="line">trans_compose=transforms.Compose([trans_resize_2,trans_totensor])</span><br><span class="line">img_resize_2=trans_compose(img_PIL)</span><br><span class="line">writer.add_image(<span class="string">&quot;resize&quot;</span>,img_resize_2,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#RandomCrop随即裁剪</span></span><br><span class="line"><span class="comment"># trans_random=transforms.RandomCrop(512)</span></span><br><span class="line">trans_random=transforms.RandomCrop((<span class="number">200</span>,<span class="number">400</span>))</span><br><span class="line">trans_compose_2=transforms.Compose([trans_random,trans_totensor])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_crop=trans_compose_2(img_PIL)</span><br><span class="line">    writer.add_image(<span class="string">&quot;RandomCrop&quot;</span>,img_crop,i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transform = transforms.Compose([</span></span><br><span class="line"><span class="comment">#     transforms.Resize((32, 32)),</span></span><br><span class="line"><span class="comment">#     transforms.ToTensor(),</span></span><br><span class="line"><span class="comment">#     transforms.Normalize((0.5,), (0.5,))</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line"><span class="comment"># train_dataset = datasets.MNIST(root=&#x27;./data&#x27;, train=True, download=True, transform=transform)</span></span><br></pre></td></tr></table></figure>

<h3 id="2-4-数据集"><a href="#2-4-数据集" class="headerlink" title="2.4 数据集"></a>2.4 数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用官方提供的数据集</span></span><br><span class="line"><span class="comment"># https://pytorch.org/vision/stable/datasets.html</span></span><br><span class="line"></span><br><span class="line">torchvision.datasets.CIFAR10(root, <span class="built_in">bool</span> = <span class="literal">True</span>, transform = <span class="literal">None</span>, target_transform = <span class="literal">None</span>, download = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># root (str or pathlib.Path):Root directory of dataset</span></span><br><span class="line"><span class="comment"># train (bool, optional): True for training set, or False for test set.</span></span><br><span class="line"><span class="comment"># transform (callable, optional) : A function/transform that takes  PIL image totensor image</span></span><br><span class="line"><span class="comment"># target_transform (callable, optional) : A function/transform that takes in the target and transforms it.</span></span><br><span class="line"><span class="comment"># download (bool, optional):true for downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line"><span class="comment"># train_set=torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,download=True)</span></span><br><span class="line"><span class="comment"># test_set=torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,download=True)</span></span><br><span class="line"></span><br><span class="line">dataset_trans=torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=dataset_trans,download=<span class="literal">True</span>)</span><br><span class="line">test_set=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=dataset_trans,download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#dataset=torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用数据</span></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(test_set.classes)</span><br><span class="line"></span><br><span class="line">img,target=test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target])</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结合tensorboard</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img,target=test_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&quot;test_set&quot;</span>,img,i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h3 id="2-5-DataLoader"><a href="#2-5-DataLoader" class="headerlink" title="2.5 DataLoader"></a>2.5 DataLoader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line"><span class="comment"># 从数据集中取数据，设置加载数据过程</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.utils.data.DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">None</span>, sampler=<span class="literal">None</span>, batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>, pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>, worker_init_fn=<span class="literal">None</span>, multiprocessing_context=<span class="literal">None</span>, generator=<span class="literal">None</span>, *, prefetch_factor=<span class="literal">None</span>, persistent_workers=<span class="literal">False</span>, pin_memory_device=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset (Dataset)：加载的数据</span></span><br><span class="line"><span class="comment"># batch_size (int, optional)：每batch加载多少数据，默认1</span></span><br><span class="line"><span class="comment"># shuffle (bool, optional)：每epoch顺序是否打乱，默认False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sampler (Sampler or Iterable, optional) – defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_sampler (Sampler or Iterable, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_workers (int, optional)：多少进程加载数据，默认0(主进程)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># collate_fn (Callable, optional) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pin_memory (bool, optional) – If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># drop_last (bool, optional):最后剩余的数据是否舍弃，默认False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># worker_init_fn (Callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据集</span></span><br><span class="line">test_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用DataLoader</span></span><br><span class="line">test_loader=DataLoader(dataset=test_data,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">img,target=test_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs,targets=data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line"><span class="comment">#     print(targets)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#drop_last不丢弃</span></span><br><span class="line">test_loader=DataLoader(dataset=test_data,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs,targets=data</span><br><span class="line">    <span class="comment"># print(imgs.shape)</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;test_data&quot;</span>,imgs,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle测试</span></span><br><span class="line">test_loader=DataLoader(dataset=test_data,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">False</span>)<span class="comment">#drop_last不丢弃</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs,targets=data</span><br><span class="line">        <span class="comment"># print(imgs.shape)</span></span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch),imgs,step)</span><br><span class="line">        step=step+<span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h2 id="3-网络结构"><a href="#3-网络结构" class="headerlink" title="3. 网络结构"></a>3. 网络结构</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pytorch官网：https://pytorch.org/docs/stable/index.html</span><br></pre></td></tr></table></figure>

<h3 id="3-1-Module"><a href="#3-1-Module" class="headerlink" title="3.1 Module"></a>3.1 Module</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># self.conv1 = nn.Conv2d(1, 20, 5)</span></span><br><span class="line">        <span class="comment"># self.conv2 = nn.Conv2d(20, 20, 5)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = F.relu(self.conv1(x))</span></span><br><span class="line">        <span class="comment"># return F.relu(self.conv2(x))</span></span><br><span class="line">        <span class="keyword">return</span> x+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">module=Model()</span><br><span class="line">x=torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">output=module(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一：逐个串联网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(<span class="variable language_">self</span>.conv2(x))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式二：Sequence快速串联网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.module=Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">            F.relu(),</span><br><span class="line">            nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">            F.relu()</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.module(x)</span><br></pre></td></tr></table></figure>

<h3 id="3-2-Convolution-Layers"><a href="#3-2-Convolution-Layers" class="headerlink" title="3.2 Convolution Layers"></a>3.2 Convolution Layers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.conv2d(<span class="built_in">input</span>, weight, bias=<span class="literal">None</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>) → Tensor</span><br><span class="line"><span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</span></span><br><span class="line">对由多个输入平面组成的输入图像应用二维卷积。卷积操作是一种数学运算，它将一个滤波器应用于图像，产生一个过滤后的输出（也称为特征图）。</span><br><span class="line"><span class="variable language_">self</span>.conv1=Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># input: 输入张量，其形状为(minibatch, in_channels, iH, iW)，其中minibatch是批量大小，in_channels是输入通道数，iH和iW分别是输入图像的高度和宽度。</span></span><br><span class="line"><span class="comment"># weight: 滤波器张量，其形状为(out_channels, in_channels // groups, kH, kW)，其中out_channels是输出通道数，in_channels // groups是每个组的输入通道数，kH和kW分别是滤波器的高度和宽度。</span></span><br><span class="line"><span class="comment"># bias: 可选的偏差张量，形状为(out_channels)。默认值为None。</span></span><br><span class="line"><span class="comment"># stride: 卷积核的步长，可以是单个数字或元组(sH, sW)。默认值为1。</span></span><br><span class="line"><span class="comment"># padding: 输入两侧的隐式填充，可以是单个数字或元组(padH, padW)。默认值为0。&#x27;valid&#x27;等同于无填充，&#x27;same&#x27;对输入进行填充，使输出与输入具有相同的形状，但此模式不支持除1以外的任何步幅值。</span></span><br><span class="line"><span class="comment"># dilation: 内核元素之间的间距，可以是单个数字或元组(dH, dW)。默认值为1。</span></span><br><span class="line"><span class="comment"># groups: 输入和输出之间的连接组数。默认值为1。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据</span></span><br><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#转换为batch_size=1，通道为1，n*n数据</span></span><br><span class="line"><span class="built_in">input</span>=torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">kernel=torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># print(input.shape)</span></span><br><span class="line"><span class="comment"># print(kernel.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.stride滑动步长</span></span><br><span class="line">output=F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">output2=F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Pandding填充</span></span><br><span class="line">output3=F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output3)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataloader=DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1=Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=<span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">module=Module()</span><br><span class="line"><span class="comment">#print(module)	# Module( (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)))</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,target=data</span><br><span class="line"></span><br><span class="line">    output=module(imgs)</span><br><span class="line">    <span class="comment">#print(imgs.shape) # torch.Size([16, 3, 32, 32])</span></span><br><span class="line">    <span class="comment">#print(output.shape)# torch.Size([16, 6, 30, 30])</span></span><br><span class="line">	writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line">    </span><br><span class="line">    output=torch.reshape(output,(-<span class="number">1</span>,<span class="number">3</span>,<span class="number">30</span>,<span class="number">30</span>)) <span class="comment"># 6通道不能显示，要转换；当不知道是多少的时候-1会自动计算</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>$$<br>Input:(N,C_{in} , H_{in},W_{in})\<br>$$</p>
<p>$$<br>Output:(N,C_{out} , H_{out},W_{out})<br>$$</p>
<p>$$<br>H_{out}&#x3D;\left \lfloor \frac{H_{in}+2\times padding[0]-dilation[0]\times \left ( kernel_size[0]-1 \right )-1 }{stride[0]} +1 \right \rfloor\<br>$$</p>
<p>$$<br>W_{out}&#x3D;\left \lfloor \frac{W_{in}+2\times padding[1]-dilation[1]\times \left ( kernel_size[1]-1 \right )-1 }{stride[1]} +1 \right \rfloor<br>$$</p>
<h3 id="3-3-Pooling-layers"><a href="#3-3-Pooling-layers" class="headerlink" title="3.3 Pooling layers"></a>3.3 Pooling layers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool1d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel_size (Union[int, Tuple[int]]) – The size of the sliding window, must be &gt; 0.</span></span><br><span class="line"><span class="comment"># stride (Union[int, Tuple[int]]) – The stride of the sliding window, must be &gt; 0. Default value is kernel_size.</span></span><br><span class="line"><span class="comment"># padding (Union[int, Tuple[int]]) – Implicit negative infinity padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2.</span></span><br><span class="line"><span class="comment"># dilation (Union[int, Tuple[int]]) – The stride between elements within a sliding window, must be &gt; 0.</span></span><br><span class="line"><span class="comment"># return_indices (bool) – If True, will return the argmax along with the max values. Useful for torch.nn.MaxUnpool1d later</span></span><br><span class="line"><span class="comment"># ceil_mode (bool) – If True, will use ceil instead of floor to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.是否要保留，默认False</span></span><br><span class="line">   </span><br><span class="line"><span class="variable language_">self</span>.maxpool1=MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">True</span>) </span><br><span class="line"><span class="variable language_">self</span>.maxpool1=MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-4-Non-linear-Activations"><a href="#3-4-Non-linear-Activations" class="headerlink" title="3.4 Non-linear Activations"></a>3.4 Non-linear Activations</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inplace (bool) ：是否结果替换原来数据，默认False</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.relu1=ReLU()</span><br></pre></td></tr></table></figure>

<h3 id="3-5-Linear-Layers"><a href="#3-5-Linear-Layers" class="headerlink" title="3.5 Linear Layers"></a>3.5 Linear Layers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classtorch.nn.Linear(in_features, out_features, bias=<span class="literal">True</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in_features (int) – size of each input sample</span></span><br><span class="line"><span class="comment"># out_features (int) – size of each output sample</span></span><br><span class="line"><span class="comment"># bias (bool) – If set to False, the layer will not learn an additive bias. Default: True</span></span><br></pre></td></tr></table></figure>

<h3 id="3-6-Others"><a href="#3-6-Others" class="headerlink" title="3.6 Others"></a>3.6 Others</h3><h4 id="1-Normalization-Layers"><a href="#1-Normalization-Layers" class="headerlink" title="1. Normalization Layers"></a>1. Normalization Layers</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正则化层，可加快训练速度，不常用</span></span><br><span class="line"><span class="comment">#https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练神经网络时往往需要标准化输入数据,使得网络的训练更加快熟和有效</span></span><br><span class="line"><span class="comment"># 但是SGD等学习算法会不断改变网络的参数,隐含层的激活值分布会发生变化(内协变量偏移ICS)</span></span><br><span class="line"><span class="comment"># 为了减轻ICS问题,BN固定激活函数的输入变量的均值和方差,使网络的训练更快</span></span><br><span class="line"><span class="comment"># 1.应用了BN的神经网络在反向传播中有较好的梯度流,这样网络对权重的均值和瓷都依赖性减少,能使用较高的学习率,降低了不收敛的风险</span></span><br><span class="line"><span class="comment"># 2.BN具有争着话的的作用,Dropout就不需要了</span></span><br><span class="line"><span class="comment"># 3.BN让深度神经网络使用饱和非线性函数成为可能</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BN训练时,使用当前批次的数据单独的估计每一激活值的均值和方差,然后利用计算好的均值和方差表遵化这一批次的激活值</span></span><br><span class="line"><span class="comment"># 为了避免除0,eps常设为非常小的数字,默认为1e-5</span></span><br><span class="line"><span class="comment">#标准化可能会降低模型的表达能力，因为有些隐含层是需要数据非标准化分布的，所以需要仿射变换</span></span><br><span class="line"></span><br><span class="line">classtorch.nn.BatchNorm1d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_features (int) ：输入的特征或通道数 C</span></span><br><span class="line"><span class="comment"># eps (float)： 为了数值稳定性而加到分母上的值。默认值：1e-5</span></span><br><span class="line"><span class="comment"># momentum (Optional[float])：用于计算 running_mean 和 running_var 的值。可以设置为 None 以使用累积移动平均（即简单平均）。默认值：0.1</span></span><br><span class="line"><span class="comment"># affine (bool)：当设置为 True 时，该模块具有可学习的仿射参数。默认值：True</span></span><br><span class="line"><span class="comment"># track_running_stats (bool)： 当设置为 True 时，该模块跟踪运行中的均值和方差，当设置为 False 时，该模块不跟踪此类统计信息，并将统计信息缓冲区 running_mean 和 running_var 初始化为 None。当这些缓冲区为 None 时，该模块在训练和评估模式下总是使用批量统计数据。默认值：True</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">m=nn.BatchNorm1d(num_features=<span class="number">5</span>,affine=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 训练前</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BEFORE&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_mean&quot;</span>,m.running_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_var&quot;</span>,m.running_var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练后</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="built_in">input</span>=torch.randn(<span class="number">20</span>,<span class="number">5</span>)</span><br><span class="line">    output=m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;AFTER&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_mean&quot;</span>,m.running_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_var&quot;</span>,m.running_var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模式</span></span><br><span class="line">m.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="built_in">input</span>=torch.randn(<span class="number">20</span>,<span class="number">5</span>)</span><br><span class="line">    output=m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;EVAL:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_mean&quot;</span>,m.running_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_var&quot;</span>,m.running_var)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># m_affine.weight，m_affine.bias被称为Parameter，即在模型中需要被训练</span></span><br><span class="line"><span class="comment"># running_mean，running_var被称为buffer，即不影响模型训练，仅仅作为中间变量更新和保存</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;no affine,gamma&quot;</span>,m.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;no affine,beta&quot;</span>,m.bias)</span><br><span class="line"></span><br><span class="line">m_affine=nn.BatchNorm1d(num_features=<span class="number">5</span>,affine=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;with affine,gamma&quot;</span>,m_affine.weight,<span class="built_in">type</span>(m_affine.weight))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;with affine,beta&quot;</span>,m_affine.bias,<span class="built_in">type</span>(m_affine.bias))</span><br></pre></td></tr></table></figure>



<h4 id="2-Transformer-Layers"><a href="#2-Transformer-Layers" class="headerlink" title="2. Transformer Layers"></a>2. Transformer Layers</h4><h4 id="3-Padding-Layers"><a href="#3-Padding-Layers" class="headerlink" title="3. Padding Layers"></a>3. Padding Layers</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于填充边框，基本用不到，可在卷积层设置</span></span><br></pre></td></tr></table></figure>

<h4 id="4-DropOut-Layers"><a href="#4-DropOut-Layers" class="headerlink" title="4. DropOut Layers"></a>4. DropOut Layers</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DropOut是一种正则化技术，通过防止特征的协同适应减少过拟合</span></span><br><span class="line"><span class="comment"># 效果好，实现简单，但不会降低网络速度</span></span><br><span class="line"><span class="comment"># 在测试时和训练时做出不同的行为,通过torch.nn.Module提供的train()和eval()方法设置为训练模式和测试模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#参数</span></span><br><span class="line"><span class="comment">#p (float) – probability of an element to be zeroed. Default: 0.5</span></span><br><span class="line"><span class="comment">#inplace (bool) – If set to True, will do this operation in-place. Default: False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#shape：</span></span><br><span class="line">Input:（*）.<span class="built_in">input</span> can be of <span class="built_in">any</span> shape</span><br><span class="line">Output:（*）.Output <span class="keyword">is</span> of the same shape  asinput</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#P的值越高，模型越精简</span></span><br><span class="line">p,iters,shape=<span class="number">0.5</span>,<span class="number">50</span>,(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">dropout=nn.Dropout(p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模式</span></span><br><span class="line">dropout.train()</span><br><span class="line">count=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    activations=torch.rand(shape)+<span class="number">1e-5</span></span><br><span class="line">    output=dropout(activations)</span><br><span class="line">    count+=torch.<span class="built_in">sum</span>(output==activations*(<span class="number">1</span>/(<span class="number">1</span>-p)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train()模式Droupout影响了&#123;&#125;个神经元&quot;</span>.<span class="built_in">format</span>(<span class="number">1</span>-<span class="built_in">float</span>(count)/(activations.nelement()*iters)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模式</span></span><br><span class="line">dropout.<span class="built_in">eval</span>()</span><br><span class="line">count=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    activations=torch.rand(shape)+<span class="number">1e-5</span></span><br><span class="line">    output=dropout(activations)</span><br><span class="line">    count+=torch.<span class="built_in">sum</span>(output==activations)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;eval()模式Droupout影响了&#123;&#125;个神经元&quot;</span>.<span class="built_in">format</span>(<span class="number">1</span> - <span class="built_in">float</span>(count) / (activations.nelement() * iters)))</span><br></pre></td></tr></table></figure>

<h2 id="4-使用网络"><a href="#4-使用网络" class="headerlink" title="4. 使用网络"></a>4. 使用网络</h2><h3 id="4-1-Loss-Function"><a href="#4-1-Loss-Function" class="headerlink" title="4.1 Loss Function"></a>4.1 Loss Function</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">module=Module()</span><br><span class="line"><span class="comment"># 选择损失函数</span></span><br><span class="line">loss=nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = module(imgs)</span><br><span class="line">    <span class="comment"># 计算损失函数</span></span><br><span class="line">    result_loss = loss(outputs, targets)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    result_loss.backward()</span><br><span class="line">    <span class="comment"># print(outputs)</span></span><br><span class="line">    <span class="comment"># print(targets)</span></span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br></pre></td></tr></table></figure>

<h3 id="4-2-Optim"><a href="#4-2-Optim" class="headerlink" title="4.2 Optim"></a>4.2 Optim</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">module=Module()</span><br><span class="line">loss=nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># optimizer = optim.Adam([var1, var2], lr=0.0001)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = module(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets)</span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        result_loss.backward()</span><br><span class="line">        optim.step()</span><br><span class="line">        running_loss=running_loss + result_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(outputs)</span></span><br><span class="line">        <span class="comment"># print(targets)</span></span><br><span class="line">        <span class="comment"># print(result_loss)</span></span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-修改现有模型"><a href="#4-3-修改现有模型" class="headerlink" title="4.3 修改现有模型"></a>4.3 修改现有模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类模型：https://pytorch.org/vision/stable/models.html#classification</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据(CIFA10数据集10个类)</span></span><br><span class="line">train_data=torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,download=<span class="string">&#x27;True&#x27;</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载网络</span></span><br><span class="line">vgg16_false=torchvision.models.vgg16(pretrained=<span class="literal">False</span>)	<span class="comment">#只有网络没有参数</span></span><br><span class="line">vgg16_true=torchvision.models.vgg16(pretrained=<span class="literal">True</span>)	<span class="comment">#加载网络和参数</span></span><br><span class="line"><span class="comment"># vgg16 = torchvision.models.vgg16(weights=models.VGG16_Weights.DEFAULT)</span></span><br><span class="line"><span class="comment">#默认是预训练的ImageNet数据集参数，分类1000个</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出VGG网络</span></span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1：另外加一个module添加一个线性层</span></span><br><span class="line">vgg16_true.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2：在最后的classifier的module中添加一个线性层</span></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式3：对最后的classifier的module中修改线性层</span></span><br><span class="line">vgg16_true.classifier[<span class="number">6</span>]=nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-4-保存与读取"><a href="#4-4-保存与读取" class="headerlink" title="4.4 保存与读取"></a>4.4 保存与读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 加载网络</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line"><span class="comment"># 保存：模型结构+模型参数</span></span><br><span class="line">torch.save(vgg16,<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">model=torch.load(<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="comment"># print(model)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 陷阱</span></span><br><span class="line"><span class="comment"># 使用自己的网络的时候，要把Module的类放到本文件</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x+<span class="number">1</span></span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line">module=Module()</span><br><span class="line">torch.save(module,<span class="string">&quot;module.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载</span></span><br><span class="line">module=torch.load(<span class="string">&quot;module.pth&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2</span></span><br><span class="line"><span class="comment"># 保存：模型参数(字典结构)（官方推荐）</span></span><br><span class="line">torch.save(vgg16.state_dict(),<span class="string">&quot;vgg16_method2.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载：先加载网络，再加载参数</span></span><br><span class="line">vgg16=torchvision.models.vgg16()</span><br><span class="line">model=vgg16.load_state_dict(torch.load(<span class="string">&quot;vgg16_method2.pth&quot;</span>))</span><br></pre></td></tr></table></figure>

<h3 id="4-5-GPU训练"><a href="#4-5-GPU训练" class="headerlink" title="4.5 GPU训练"></a>4.5 GPU训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1：网络模型、损失函数、数据(输入，标注)的cuda方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在实例化模型后面添加</span></span><br><span class="line"><span class="comment"># module = Module()</span></span><br><span class="line">module = module.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在损失函数后面添加</span></span><br><span class="line"><span class="comment"># loss_fn = nn.CrossEntropyLoss()</span></span><br><span class="line">loss_fn = loss_fn.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在数据后面添加</span></span><br><span class="line"><span class="comment"># for i in range(epoch):</span></span><br><span class="line"><span class="comment">#     print(&quot;------------第&#123;&#125;轮训练开始-------------&quot;.format(i + 1))</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     # 训练步骤开始</span></span><br><span class="line"><span class="comment">#     module.train()  # 只对dropout层和BatchNorm有用</span></span><br><span class="line"><span class="comment">#     for data in train_dataloader:</span></span><br><span class="line"><span class="comment">#         imgs, targets = data</span></span><br><span class="line">          imgs = imgs.cuda()</span><br><span class="line">          targets = targets.cuda()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 有时需要根据情况，需要判断该设备有没有GPU，需添加判断条件</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    module = module.cuda()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2：网络模型、损失函数、数据(输入，标注)的to(device)方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的设备</span></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># torch.device(&quot;cpu&quot;)#使用CPU训练</span></span><br><span class="line"><span class="comment"># torch.device(&quot;cuda&quot;)#使用GPU训练</span></span><br><span class="line"><span class="comment"># torch.device(&quot;cuda:0&quot;)#使用第一个GPU训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在实例化模型后面添加</span></span><br><span class="line"><span class="comment"># module = Module()</span></span><br><span class="line">module = module.to(device)</span><br><span class="line"><span class="comment"># module.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在损失函数后面添加</span></span><br><span class="line"><span class="comment"># loss_fn = nn.CrossEntropyLoss()</span></span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"><span class="comment"># loss_fn.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在数据后面添加</span></span><br><span class="line"><span class="comment"># for i in range(epoch):</span></span><br><span class="line"><span class="comment">#     print(&quot;------------第&#123;&#125;轮训练开始-------------&quot;.format(i + 1))</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     # 训练步骤开始</span></span><br><span class="line"><span class="comment">#     module.train()  # 只对dropout层和BatchNorm有用</span></span><br><span class="line"><span class="comment">#     for data in train_dataloader:</span></span><br><span class="line"><span class="comment">#         imgs, targets = data</span></span><br><span class="line">          imgs = imgs.to(device)</span><br><span class="line">          targets = targets.to(device)</span><br></pre></td></tr></table></figure>

<h3 id="4-6-计算时间"><a href="#4-6-计算时间" class="headerlink" title="4.6 计算时间"></a>4.6 计算时间</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当实验需要计算时间</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练开始前设置开始时间</span></span><br><span class="line">start_time=time.time()</span><br><span class="line"><span class="comment"># for i in range(epoch):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在需要结束的时间设置结束时间</span></span><br><span class="line">end_time=time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算时间差</span></span><br><span class="line"><span class="built_in">print</span>(end_time - start_time)</span><br></pre></td></tr></table></figure>

<h3 id="4-7-模型训练套路"><a href="#4-7-模型训练套路" class="headerlink" title="4.7 模型训练套路"></a>4.7 模型训练套路</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0. 搭建神经网络module.py</span></span><br><span class="line">form torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=<span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    module=Module()</span><br><span class="line">    <span class="built_in">input</span>=torch.ones(<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">    output=module(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 准备数据集</span></span><br><span class="line">train_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">test_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获得length长度</span></span><br><span class="line">train_data_size=<span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size=<span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># print(&quot;训练数据集的长度为:&#123;&#125;&quot;.format(train_data_size))</span></span><br><span class="line"><span class="comment"># print(&quot;测试数据集的长度为:&#123;&#125;&quot;.format(test_data_size))</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>) <span class="comment">#shuffle=True</span></span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)	<span class="comment">#defaule shuffle=False</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 加载模型和超参数</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的设备</span></span><br><span class="line"><span class="comment"># device=torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">module = Module()</span><br><span class="line"><span class="comment"># module.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># loss_fn.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line"><span class="comment"># learning_rate = 1e-2</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(module.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的参数</span></span><br><span class="line"><span class="comment"># 记录训练次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. 开始训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------第&#123;&#125;轮训练开始-------------&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    <span class="comment"># module.train()  # 只对dropout层和BatchNorm有用</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        <span class="comment"># imgs=imgs.to(device)</span></span><br><span class="line">        <span class="comment"># targets=targets.to(device)</span></span><br><span class="line">        output = module(imgs)</span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每到100次输出训练情况</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>((total_train_step), loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    <span class="comment"># module.eval() # 只对dropout层和BatchNorm有用</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            <span class="comment"># imgs=imgs.to(device)</span></span><br><span class="line">            <span class="comment"># targets=targets.to(device)</span></span><br><span class="line"></span><br><span class="line">            outputs = module(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss = total_test_loss + loss</span><br><span class="line"></span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集的Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集的正确率:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy / test_data_size))</span><br><span class="line"></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy / test_data_size, total_test_step)</span><br><span class="line"></span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    torch.save(module, <span class="string">&quot;model_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))	<span class="comment">#方式1保存</span></span><br><span class="line">    <span class="comment"># torch.save(module.state_dict(),&quot;model_&#123;&#125;.pth&quot;.format(i))	#方式2保存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LeNet-5模型</span></span><br><span class="line"><span class="comment">#class Module(nn.Module):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载MNIST数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_dataset, batch_size=<span class="number">1000</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型、损失函数和优化器</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = Module().to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 训练10个epoch</span></span><br><span class="line">    model.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        images, labels = images.to(device), labels.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;running_loss / <span class="built_in">len</span>(train_loader)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images, labels = images.to(device), labels.to(device)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;<span class="number">100</span> * correct / total&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="4-8-模型验证讨论"><a href="#4-8-模型验证讨论" class="headerlink" title="4.8 模型验证讨论"></a>4.8 模型验证讨论</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用已经训练好的模型进行应用</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms.v2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提供的神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=<span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载测试图片</span></span><br><span class="line">img_path=<span class="string">&quot;./images/dog.jpg&quot;</span></span><br><span class="line">image=Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(image)</span></span><br><span class="line"><span class="comment"># image=image.convert(&#x27;RGB&#x27;)</span></span><br><span class="line">transform=torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="number">32</span>,<span class="number">32</span>)),</span><br><span class="line">                                          torchvision.transforms.ToTensor()])</span><br><span class="line">image=transform(image)</span><br><span class="line"><span class="comment"># print(image.shape)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model = torch.load(<span class="string">&quot;model_9.pth&quot;</span>)</span><br><span class="line"><span class="comment"># model = torch.load(&quot;model_9.pth&quot;, map_location=torch.device(&#x27;cpu&#x27;))#GPU训练参数用在CPU上</span></span><br><span class="line"><span class="comment"># print(model)</span></span><br><span class="line">image = torch.reshape(image, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)) <span class="comment">#模型需要4维输入</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(image)</span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"><span class="built_in">print</span>(output.argmax(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h1 id="二、pytorch深度学习实践"><a href="#二、pytorch深度学习实践" class="headerlink" title="二、pytorch深度学习实践"></a>二、pytorch深度学习实践</h1><h2 id="1-实现Logistic回归"><a href="#1-实现Logistic回归" class="headerlink" title="1. 实现Logistic回归"></a>1. 实现Logistic回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># %matplotlib inline</span></span><br></pre></td></tr></table></figure>

<h3 id="1-1-生成数据"><a href="#1-1-生成数据" class="headerlink" title="1.1 生成数据"></a>1.1 生成数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置两个高斯分布的均值向量和协方差矩阵</span></span><br><span class="line">mu1 =-<span class="number">3</span>*torch.ones(<span class="number">2</span>)</span><br><span class="line">mu2 =<span class="number">3</span>*torch.ones(<span class="number">2</span>)</span><br><span class="line">sigma1=torch.eye(<span class="number">2</span>)*<span class="number">0.5</span></span><br><span class="line">sigma2=torch.eye(<span class="number">2</span>)*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从两个多元高斯分布中生成100个样本</span></span><br><span class="line">m1=MultivariateNormal(mu1,sigma1)</span><br><span class="line">m2=MultivariateNormal(mu2,sigma2)</span><br><span class="line">x1=m1.sample((<span class="number">100</span>,))</span><br><span class="line">x2=m2.sample((<span class="number">100</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置正负样本的标签</span></span><br><span class="line">y=torch.zeros((<span class="number">200</span>,<span class="number">1</span>))</span><br><span class="line">y[<span class="number">100</span>:]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#组合、打乱样本</span></span><br><span class="line">x=torch.cat([x1,x2],dim=<span class="number">0</span>)</span><br><span class="line">idx=np.random.permutation(<span class="built_in">len</span>(x))</span><br><span class="line">x=x[idx]</span><br><span class="line">y=y[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制样本</span></span><br><span class="line">plt.scatter(x1.numpy()[:,<span class="number">0</span>],x1.numpy()[:,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x2.numpy()[:,<span class="number">0</span>],x2.numpy()[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h3 id="1-2-线性方程"><a href="#1-2-线性方程" class="headerlink" title="1.2 线性方程"></a>1.2 线性方程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.nn.Linear中的函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置输入维度和输出维度，D_in 是输入的特征维度，D_out 是输出的特征维度</span></span><br><span class="line">D_in, D_out = <span class="number">2</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个线性层，并且使用偏置 (bias=True)</span></span><br><span class="line">linear = nn.Linear(D_in, D_out, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用线性层对输入 x 进行线性变换，计算输出，x 是大小为 (200, 2) 的输入</span></span><br><span class="line">output = linear(x) <span class="comment">#output 是 nn.Linear 层的输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以打印输入 x 的形状、线性层的权重形状、偏置项的值、以及输出的形状</span></span><br><span class="line"><span class="built_in">print</span>(x.shape,            <span class="comment"># (200, 2)，表示 200 行 2 列（200 个样本，每个样本有 2 个特征）</span></span><br><span class="line">      linear.weight.shape, <span class="comment"># (1, 2)，线性层的权重矩阵，输出维度 1，输入维度 2</span></span><br><span class="line">      linear.bias,         <span class="comment"># 线性层的偏置项，大小为 (1,)，是一个常数偏置</span></span><br><span class="line">      output.shape)        <span class="comment"># (200, 1)，表示 200 行 1 列（200 个样本，每个样本对应 1 个输出值）</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用自定义的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_linear</span>(<span class="params">x, w, b</span>):</span><br><span class="line">    <span class="comment"># 使用矩阵乘法进行线性变换，x 大小为 (200, 2)，w.t() 是权重矩阵 w 的转置，大小为 (2, 1)</span></span><br><span class="line">    <span class="comment"># torch.mm() 是 PyTorch 中的矩阵乘法函数，最后加上偏置项 b，大小为 (1,)</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(x, w.t()) + b</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 PyTorch 线性层输出与自定义线性函数输出之间的差异</span></span><br><span class="line"><span class="comment"># output 是 nn.Linear 层的输出，my_linear(x, linear.weight, linear.bias) 是自定义线性函数的输出</span></span><br><span class="line"><span class="comment"># 两者的差异应该非常接近于 0</span></span><br><span class="line">difference = torch.<span class="built_in">sum</span>((output - my_linear(x, linear.weight, linear.bias)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印差异，理想情况下应该为 0 或非常接近 0，表示两种计算方法一致</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;difference:&quot;</span> ,difference)</span><br></pre></td></tr></table></figure>

<h3 id="1-3-激活函数"><a href="#1-3-激活函数" class="headerlink" title="1.3 激活函数"></a>1.3 激活函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.nn.Sigmoid激活函数</span></span><br><span class="line"><span class="comment"># 创建一个 Sigmoid 激活函数</span></span><br><span class="line">sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对线性层的输出应用 Sigmoid 激活函数，得到分类分数</span></span><br><span class="line">scores = sigmoid(output)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用自定义的Sigmoid激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># Sigmoid 函数的定义：1 / (1 + exp(-x))</span></span><br><span class="line">    x = <span class="number">1</span> / (<span class="number">1</span> + torch.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 PyTorch 的 Sigmoid 函数与自定义 Sigmoid 函数之间的差异</span></span><br><span class="line">difference = torch.<span class="built_in">sum</span>(sigmoid(output) - sigmoid(output))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印差异，理想情况下应该为 0 或非常接近 0，表示两种计算方法一致</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;difference:&quot;</span> ,difference)</span><br></pre></td></tr></table></figure>

<h3 id="1-4-损失函数"><a href="#1-4-损失函数" class="headerlink" title="1.4 损失函数"></a>1.4 损失函数</h3><p>Logistic函数使用交叉熵作为损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.nn提供的交叉熵函数</span></span><br><span class="line"><span class="comment"># 创建一个二值交叉熵损失函数 (Binary Cross Entropy Loss)</span></span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对经过 Sigmoid 激活后的输出和真实标签 y 计算损失</span></span><br><span class="line">bce_loss = loss(sigmoid(output), y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用自定义的二值交叉熵函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="comment"># BCELoss 的公式：- [y*log(x) + (1-y)*log(1-x)] 的平均值</span></span><br><span class="line">    <span class="comment"># 为避免数值问题，x 的值需要限制在 (0,1) 范围内，因此使用 log()。</span></span><br><span class="line">    loss = -torch.mean(torch.log(x) * y + torch.log(<span class="number">1</span> - x) * (<span class="number">1</span> - y))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算 PyTorch 的交叉熵损失与自定义交叉熵损失之间的差异</span></span><br><span class="line">loss_difference = loss(sigmoid(output), y) - my_loss(sigmoid(output), y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印两种损失函数之间的差异，理想情况下应该为 0 或非常接近 0，表示两者计算方法一致</span></span><br><span class="line"><span class="built_in">print</span>(loss_difference)</span><br></pre></td></tr></table></figure>

<h3 id="1-5-nn-Module整合2-4"><a href="#1-5-nn-Module整合2-4" class="headerlink" title="1.5 nn.Module整合2-4"></a>1.5 nn.Module整合2-4</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#由于torch.nn包中的nn.Linear、nn.Sigmoid、nn.BCELoss都继承于nn.Module类，因此可通过继承nn.Module构建模型</span></span><br><span class="line"><span class="comment">#通过nn.module实现自己的模型时，forward()方法是必须被子类复写，在forward()内部定义每次调用模型时执行的计算</span></span><br><span class="line"><span class="comment">#nn.Module类的作用是接收Tensor然后返回结果</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegression, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个线性层，输入维度为 D_in，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(D_in, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义 Sigmoid 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将输入 x 传递给线性层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.linear(x)</span><br><span class="line">        <span class="comment"># 将线性层的输出传递给 Sigmoid 激活函数</span></span><br><span class="line">        output = <span class="variable language_">self</span>.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 实例化 LogisticRegression 模型，输入维度为 2</span></span><br><span class="line">lr_model = LogisticRegression(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义二分类交叉熵损失函数</span></span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型输出的损失，传入经过 Sigmoid 处理的输出和真实标签 y</span></span><br><span class="line">output = lr_model(x)</span><br><span class="line">loss = loss_fn(output, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印损失值</span></span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在Module中，还可以嵌套其他的Module，被嵌套的Module的属性可以被自动获取</span></span><br><span class="line"><span class="comment">#比如调用nn.Module.parameters()获取Module的全部参数</span></span><br><span class="line"><span class="comment">#nn.Module.to()讲模型的参数放到GPU上</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义两个线性层，输入和输出都是 1 维</span></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(<span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(<span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个空的 forward 函数，虽然 forward 未实现，但这不会影响参数的打印</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例并遍历模型中的参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> MyModel().parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure>

<h3 id="1-6-优化算法"><a href="#1-6-优化算法" class="headerlink" title="1.6 优化算法"></a>1.6 优化算法</h3><p>Logistic回归使用梯度下降算法优化目标参数</p>
<p>PyTorch的torch.optim包实现了大多数的优化算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment">#首先构建一个优化器，构建时需将带待学习的参数传入，然后传入优化器需要的参数(比如学习率)</span></span><br><span class="line"><span class="comment"># 使用随机梯度下降（SGD）优化器来优化 lr_model 的参数，学习率设为 0.03</span></span><br><span class="line">optimizer = optim.SGD(lr_model.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建完学习率就可以迭代的对模型进行训练</span></span><br><span class="line"><span class="comment">#步骤一：调用loss损失函数的backward()方法计算模型的梯度</span></span><br><span class="line"><span class="comment">#步骤二：然后再调用优化器的step方法更新模型的参数</span></span><br><span class="line"><span class="comment">#注意：应当调用优化器的zero_grad()方法清空参数的梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 定义迭代次数</span></span><br><span class="line">iters = <span class="number">10</span></span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">#for input,target in dataset:</span></span><br><span class="line"><span class="comment"># 开始训练模型的循环，进行 10 次迭代</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    <span class="comment"># 将数据分成大小为 batch_size 的小批次进行训练</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(<span class="built_in">len</span>(x) / batch_size)):</span><br><span class="line">        <span class="comment"># 获取当前批次的输入数据 input 和标签 target</span></span><br><span class="line">        <span class="built_in">input</span> = x[i * batch_size : (i + <span class="number">1</span>) * batch_size]</span><br><span class="line">        target = y[i * batch_size : (i + <span class="number">1</span>) * batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在进行反向传播之前，清除上一次计算的梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用模型 lr_model 对当前批次的输入进行前向传播，得到预测输出</span></span><br><span class="line">        output = lr_model(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算当前批次的损失 l</span></span><br><span class="line">        l = loss(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过反向传播计算梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新模型的参数，使其沿着梯度下降方向优化</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>

<h3 id="1-7-可视化"><a href="#1-7-可视化" class="headerlink" title="1.7 可视化"></a>1.7 可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型可视化</span></span><br><span class="line"><span class="comment">#output=lr_model(x)</span></span><br><span class="line"></span><br><span class="line">pred_neg=(output&lt;=<span class="number">0.5</span>).view(-<span class="number">1</span>)</span><br><span class="line">pred_pos=(output&gt;<span class="number">0.5</span>).view(-<span class="number">1</span>)</span><br><span class="line">plt.scatter(x[pred_neg,<span class="number">0</span>],x[pred_neg,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[pred_pos,<span class="number">0</span>],x[pred_pos,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">w=lr_model.linear.weight[<span class="number">0</span>]</span><br><span class="line">b=lr_model.linear.bias[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_decision_boundary</span>(<span class="params">w,b,x0</span>):</span><br><span class="line">    x1=(-b-w[<span class="number">0</span>]*x0)/w[<span class="number">1</span>]</span><br><span class="line">    plt.plot(x0.detach().numpy(),x1.detach().numpy(),<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">draw_decision_boundary(w,b,torch.linspace(x.<span class="built_in">min</span>(),x.<span class="built_in">max</span>(),<span class="number">50</span>))</span><br></pre></td></tr></table></figure>

<h3 id="1-8-整体代码"><a href="#1-8-整体代码" class="headerlink" title="1.8 整体代码"></a>1.8 整体代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 生成数据</span></span><br><span class="line"><span class="comment">#设置两个高斯分布的均值向量和协方差矩阵</span></span><br><span class="line">mu1 =-<span class="number">3</span>*torch.ones(<span class="number">2</span>)</span><br><span class="line">mu2 =<span class="number">3</span>*torch.ones(<span class="number">2</span>)</span><br><span class="line">sigma1=torch.eye(<span class="number">2</span>)*<span class="number">0.5</span></span><br><span class="line">sigma2=torch.eye(<span class="number">2</span>)*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从两个多元高斯分布中生成100个样本</span></span><br><span class="line">m1=MultivariateNormal(mu1,sigma1)</span><br><span class="line">m2=MultivariateNormal(mu2,sigma2)</span><br><span class="line">x1=m1.sample((<span class="number">100</span>,))</span><br><span class="line">x2=m2.sample((<span class="number">100</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置正负样本的标签</span></span><br><span class="line">y=torch.zeros((<span class="number">200</span>,<span class="number">1</span>))</span><br><span class="line">y[<span class="number">100</span>:]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#组合、打乱样本</span></span><br><span class="line">x=torch.cat([x1,x2],dim=<span class="number">0</span>)</span><br><span class="line">idx=np.random.permutation(<span class="built_in">len</span>(x))</span><br><span class="line">x=x[idx]</span><br><span class="line">y=y[idx]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 定义网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegression, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个线性层，输入维度为 D_in，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(D_in, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义 Sigmoid 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将输入 x 传递给线性层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.linear(x)</span><br><span class="line">        <span class="comment"># 将线性层的输出传递给 Sigmoid 激活函数</span></span><br><span class="line">        output = <span class="variable language_">self</span>.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 设置参数</span></span><br><span class="line"><span class="comment"># 实例化 LogisticRegression 模型，输入维度为 2</span></span><br><span class="line">lr_model = LogisticRegression(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 使用优化器,随机梯度下降算法，学习率设为 0.03</span></span><br><span class="line">optimizer = optim.SGD(lr_model.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="comment"># 定义每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 定义迭代次数</span></span><br><span class="line">iters = <span class="number">10</span></span><br><span class="line"><span class="comment"># 定义二分类交叉熵损失函数</span></span><br><span class="line">loss = nn.BCELoss()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. 训练模型</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    <span class="comment"># 将数据分成大小为 batch_size 的小批次进行训练</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(<span class="built_in">len</span>(x) / batch_size)):</span><br><span class="line">        <span class="comment"># 获取当前批次的输入数据 input 和标签 target</span></span><br><span class="line">        <span class="built_in">input</span> = x[i * batch_size : (i + <span class="number">1</span>) * batch_size]</span><br><span class="line">        target = y[i * batch_size : (i + <span class="number">1</span>) * batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在进行反向传播之前，清除上一次计算的梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 使用模型 lr_model 对当前批次的输入进行前向传播，得到预测输出</span></span><br><span class="line">        output = lr_model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># 计算当前批次的损失 l</span></span><br><span class="line">        l = loss(output, target)</span><br><span class="line">        <span class="comment"># 通过反向传播计算梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment"># 更新模型的参数，使其沿着梯度下降方向优化</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. 模型可视化</span></span><br><span class="line">output=lr_model(x)</span><br><span class="line">pred_neg=(output&lt;=<span class="number">0.5</span>).view(-<span class="number">1</span>)</span><br><span class="line">pred_pos=(output&gt;<span class="number">0.5</span>).view(-<span class="number">1</span>)</span><br><span class="line">plt.scatter(x[pred_neg,<span class="number">0</span>],x[pred_neg,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[pred_pos,<span class="number">0</span>],x[pred_pos,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">w=lr_model.linear.weight[<span class="number">0</span>]</span><br><span class="line">b=lr_model.linear.bias[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_decision_boundary</span>(<span class="params">w,b,x0</span>):</span><br><span class="line">    x1=(-b-w[<span class="number">0</span>]*x0)/w[<span class="number">1</span>]</span><br><span class="line">    plt.plot(x0.detach().numpy(),x1.detach().numpy(),<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">draw_decision_boundary(w,b,torch.linspace(x.<span class="built_in">min</span>(),x.<span class="built_in">max</span>(),<span class="number">50</span>))</span><br></pre></td></tr></table></figure>

<h1 id="三、现有网络"><a href="#三、现有网络" class="headerlink" title="三、现有网络"></a>三、现有网络</h1><h2 id="1-LeNet-5"><a href="#1-LeNet-5" class="headerlink" title="1. LeNet-5"></a>1. LeNet-5</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">机器学习模型：Yann LeCun 及其同事在 1998 年开发的卷积神经网络（CNN）架构，主要用于手写数字识别(MNIST 数据集)</span><br><span class="line">主要内容：卷积、平均池化、tanh激活函数、全连接</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241107160241.png" alt="截图_20241107160241"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">输入层：</span><br><span class="line">LeNet-<span class="number">5</span> 的输入为 32x32 像素的灰度图像。</span><br><span class="line"></span><br><span class="line">第一个卷积层和池化层：</span><br><span class="line">C1 层：第一个卷积层，应用 <span class="number">6</span> 个 5x5 的卷积核，产生 <span class="number">6</span> 个 28x28 的特征图，并使用 tanh 激活函数。</span><br><span class="line">S2 层：第一个下采样（平均池化）层，使用 2x2 的窗口和步幅为 <span class="number">2</span>，将特征图缩小到 14x14。</span><br><span class="line"></span><br><span class="line">第二个卷积和池化层：</span><br><span class="line">C3 层：另一个卷积层，使用 <span class="number">16</span> 个 5x5 的卷积核，得到 <span class="number">16</span> 个 10x10 的特征图，卷积连接并不是完全连接的，从而增加了计算效率和选择性学习。</span><br><span class="line">S4 层：一个下采样层，将特征图尺寸减少到 5x5。</span><br><span class="line"></span><br><span class="line">全连接层：</span><br><span class="line">C5 层：一个全连接的卷积层，包含 <span class="number">120</span> 个节点，每个节点与上一层 5x5 输入节点全部连接。</span><br><span class="line">F6 层：另一个全连接层，包含 <span class="number">84</span> 个单元，通常使用 tanh 激活函数。</span><br><span class="line"></span><br><span class="line">输出层：</span><br><span class="line">最终层是一个包含 <span class="number">10</span> 个单元的 softmax 分类器（用于区分 <span class="number">0</span>-<span class="number">9</span> 的手写数字）。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class LeNet5(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(LeNet5, self).__init__()</span><br><span class="line">        # 卷积层1: 输入1通道(灰度图)，输出6个特征图，卷积核大小为5x5</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)</span><br><span class="line">        # 卷积层2: 输入6通道，输出16通道，卷积核大小为5x5</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)</span><br><span class="line">        # 全连接层: 输入16*5*5个节点，输出120个节点</span><br><span class="line">        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)</span><br><span class="line">        # 全连接层: 输入120个节点，输出84个节点</span><br><span class="line">        self.fc2 = nn.Linear(in_features=120, out_features=84)</span><br><span class="line">        # 最终输出层: 输入84个节点，输出10个节点（对应数字0-9的分类）</span><br><span class="line">        self.fc3 = nn.Linear(in_features=84, out_features=10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 第一层卷积 + 激活 + 平均池化</span><br><span class="line">        x = F.tanh(self.conv1(x))</span><br><span class="line">        x = F.avg_pool2d(x, kernel_size=2, stride=2)</span><br><span class="line">        # 第二层卷积 + 激活 + 平均池化</span><br><span class="line">        x = F.tanh(self.conv2(x))</span><br><span class="line">        x = F.avg_pool2d(x, kernel_size=2, stride=2)</span><br><span class="line">        # 展平操作</span><br><span class="line">        x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">        # 全连接层1 + 激活</span><br><span class="line">        x = F.tanh(self.fc1(x))</span><br><span class="line">        # 全连接层2 + 激活</span><br><span class="line">        x = F.tanh(self.fc2(x))</span><br><span class="line">        # 输出层</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 数据预处理</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((32, 32)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((0.5,), (0.5,))</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>层</th>
<th>卷积核数量</th>
<th>卷积核大小</th>
<th>步长</th>
<th>填充</th>
<th>特征图大小</th>
<th>参数</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>输入层</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>32×32×1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Conv1</td>
<td>6</td>
<td>5×5</td>
<td>1</td>
<td>0</td>
<td>28×28×6</td>
<td>156</td>
<td>包含偏置</td>
</tr>
<tr>
<td>Polling1</td>
<td></td>
<td>2×2</td>
<td>2</td>
<td>0</td>
<td>14×14×6</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>Conv2</td>
<td>16</td>
<td>5×5</td>
<td>1</td>
<td>0</td>
<td>10×10×16</td>
<td>1516</td>
<td></td>
</tr>
<tr>
<td>Polling2</td>
<td></td>
<td>2×2</td>
<td>2</td>
<td>0</td>
<td>5×5×16</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>Conv3</td>
<td>120</td>
<td>5×5</td>
<td>1</td>
<td>0</td>
<td>120×1×1</td>
<td>48120</td>
<td></td>
</tr>
<tr>
<td>FC1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>84</td>
<td>10164</td>
<td></td>
</tr>
<tr>
<td>FC2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>10</td>
<td>840</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>参数合计</td>
<td>60796</td>
<td></td>
</tr>
</tbody></table>
<h2 id="2-AlexNet"><a href="#2-AlexNet" class="headerlink" title="2. AlexNet"></a>2. AlexNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">AlexNet是更深、更大的LeNet,获得了2012lmageNet竞赛冠军</span><br><span class="line">论文网址：https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</span><br><span class="line">主要改进：</span><br><span class="line">1. 丢弃法Dropout，解决过拟合问题，模型的控制</span><br><span class="line">2. 激活函数ReLU，让特征更明显，获得更大的梯度，支持更深的模型(解决过快的产生梯度弥散问题)</span><br><span class="line">3. 最大池化MaxPooling，得到特征更大，即梯度更大，支持更深的模型</span><br><span class="line">4. 数据增强</span><br><span class="line">5. 多GPU训练：受限于硬件，将网络放到两个GPU上并行运算，由于复杂性往后不在使用，改为基于Batch的多数据并行(数据并行)</span><br><span class="line">多种优化策略的整合，包括：数据增广、数据预处理（减均值）、Dropout、有重叠池化、非线性激活ReLU、局部正则响应、多GPU训练、多模型融合等；</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241108192915.png" alt="截图_20241108192915"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义AlexNet模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.features = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层1</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 卷积层2</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 卷积层3</span></span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># 卷积层4</span></span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># 卷积层5</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>)  <span class="comment"># 展平</span></span><br><span class="line">        x = <span class="variable language_">self</span>.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 数据预处理</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((224, 224)),  # 将输入图像调整为 224x224</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用ImageNet的均值和标准差进行标准化</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h2 id="3-VGG"><a href="#3-VGG" class="headerlink" title="3. VGG"></a>3. VGG</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">结构简单(模块化)，易于拓展</span><br><span class="line">2014年，ImageNet大规模视觉识别挑战赛（ILSVRC）</span><br><span class="line">论文网址：https://arxiv.org/pdf/1409.1556</span><br><span class="line">1. 全部采用3*3卷积核（步长、填充为1）</span><br><span class="line">堆叠更多的3*3的卷积比少量的5*5效果更好</span><br><span class="line">更深的模型比更大的卷积核效果更好</span><br><span class="line">增加了更多的非线性，提高了网络的拟合/表达能力</span><br><span class="line">减少了参数的数量</span><br><span class="line"></span><br><span class="line">2. 采用堆叠卷积层的方法构建卷积层组，来保证较大的感受野</span><br><span class="line">通过卷积层实现特征图的尺寸缩是线性的</span><br><span class="line">卷积层只关注特征提取，尺度缩放留给池化层</span><br><span class="line"></span><br><span class="line">3. 全部使用2×2的最大池化核</span><br><span class="line"></span><br><span class="line">更强的非线性：1to3 -&gt; 更深的网络</span><br><span class="line">更少的参数：3*（3^2C）=27C vs 7^2C=49C,C通道数</span><br><span class="line">更多的通道数：更多的特征</span><br><span class="line">更小的卷积核：更大的特征图，更多的信息</span><br><span class="line"></span><br><span class="line">VGG支持任意尺度的测试：</span><br><span class="line">无论是Alexnet还是VGGNet都有一个问题，即无法实现任意尺度的训练/推理。(输入控制一定的n*n)</span><br><span class="line">卷积层和全连接层的运算都是矩阵点乘，因此运算规则上没有发生变化</span><br><span class="line">非标准尺度的输出是一个Scoremap，而不是一个值。需要对其求平均才能获得one-hot标签向量</span><br><span class="line">训练集采用相同的尺寸，测试集可用不同的尺寸</span><br><span class="line"></span><br><span class="line">包括：VGG16，VGG19</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241108194707.png" alt="截图_20241108194707"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.features = features</span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># 展平</span></span><br><span class="line">        x = <span class="variable language_">self</span>.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建VGG网络层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_layers</span>(<span class="params">cfg, batch_norm=<span class="literal">False</span></span>):</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> batch_norm:</span><br><span class="line">                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [conv2d, nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># VGG配置</span></span><br><span class="line">cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;VGG11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建VGG16模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg16</span>(<span class="params">num_classes=<span class="number">1000</span>, batch_norm=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">return</span> VGG(make_layers(cfg[<span class="string">&#x27;VGG16&#x27;</span>], batch_norm=batch_norm), num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = vgg16(num_classes=10).to(device)  # CIFAR-10有10个类别</span></span><br></pre></td></tr></table></figure>

<h2 id="4-NiN-Net"><a href="#4-NiN-Net" class="headerlink" title="4. NiN Net"></a>4. NiN Net</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">大多数参数来源于全连接层以及全连接层和卷积层的边界</span><br><span class="line">NiN块抛弃全连接层，只保留卷积层</span><br><span class="line">论文地址：https://arxiv.org/pdf/1312.4400</span><br><span class="line">全局平均池化”层，它能够显著减少模型参数的数量，并有助于减轻过拟合</span><br></pre></td></tr></table></figure>



<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241108204422.png" alt="截图_20241108204422"></p>
<h2 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2014年，22层</span><br><span class="line">Inception V1块，使用不同的卷积块</span><br><span class="line">论文网址：https://arxiv.org/pdf/1409.4842</span><br><span class="line">按通道将不同通道的输出进行连接（不改变高宽，只改变通道数）</span><br><span class="line">输出与输入的高宽尺度相同</span><br><span class="line">不同的卷积窗口提取不同维度的特征</span><br><span class="line"></span><br><span class="line">Inception模块是一种精心设计的网络拓扑结构</span><br><span class="line">GoogLeNet由5组，共9个lnception块组成，深度22层</span><br><span class="line"></span><br><span class="line">堆叠具有降维功能的Inception模块可以构建计算量更低的GoogLeNet，只有5百万的参数</span><br><span class="line">比AlexNet少12倍</span><br><span class="line">比VGG16少16倍</span><br><span class="line"></span><br><span class="line">全局平均池化</span><br></pre></td></tr></table></figure>



<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241108211824.png" alt="截图_20241108211824"></p>
<h2 id="6-ResNet"><a href="#6-ResNet" class="headerlink" title="6. ResNet"></a>6. ResNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">2015年，深度残差网络（超过100层，何凯明）</span><br><span class="line">论文网址：https://arxiv.org/pdf/1512.03385</span><br><span class="line">每个卷积层后都使用Batch Normalization批正则化</span><br><span class="line">使用Xavier（Heetal.）替换随机初始化</span><br><span class="line">优化方法：SGD+Momentum0.9）</span><br><span class="line">学习率：0.1起步（Worm up），验证误差不再改变后缩小10倍</span><br><span class="line">Mini-batch：256，L2权重衰减系数：1e-5</span><br><span class="line">最后一个卷积层后面紧跟一个全局平均池化层</span><br><span class="line">没有使用dropout（除分类器，没有额外的FC层）</span><br><span class="line">所有模型都从头开始训练（from scratch），未使用微调训练</span><br><span class="line">标准化的超参数（Hyper-parameters）和数据增广（Augmentation）</span><br><span class="line"></span><br><span class="line">1.残差块有效缓解了梯度弥散带来的训练困难的问题很深的网络更加容易训</span><br><span class="line">现了超过1000层的训练。</span><br><span class="line">2.瓶颈结构进一步提高了残差结构的性能</span><br><span class="line">3.间歇性（组）地实现卷积特征图数量×2以及尺度减半（stride=2）</span><br><span class="line">4.残差网络保持了网络“设计”的简约性，使用VGG-Style，所有卷积组都是</span><br><span class="line">3×3和1×1卷积，结构简单，无全连接层、无Dropout</span><br><span class="line">5.ResNet通过堆叠残差模块构建网络，性能优异，横扫了ILSVRC15和</span><br><span class="line">COCO15竞赛中所有的分类和检测任务，并超越了人的识别能力。</span><br><span class="line">残差网络对随后的深层神经网络设计产生了深远影响</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241108215824.png" alt="截图_20241108215824"></p>
<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241108220100.png" alt="截图_20241108220100"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 瓶颈块，通过 1x1、3x3 和 1x1 卷积实现降维、处理特征和升维</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], first=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride[<span class="number">0</span>], padding=padding[<span class="number">0</span>], bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride[<span class="number">1</span>], padding=padding[<span class="number">1</span>], bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels * <span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=stride[<span class="number">2</span>], padding=padding[<span class="number">2</span>], bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels * <span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 由于存在维度不一致的情况 所以分情况</span></span><br><span class="line">        <span class="variable language_">self</span>.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> first:</span><br><span class="line">            <span class="variable language_">self</span>.shortcut = nn.Sequential(</span><br><span class="line">                <span class="comment"># 卷积核为1 进行升降维</span></span><br><span class="line">                <span class="comment"># 注意跳变时 都是stride==2的时候 也就是每次输出信道升维的时候</span></span><br><span class="line">                nn.Conv2d(in_channels, out_channels * <span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=stride[<span class="number">1</span>], bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels * <span class="number">4</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.bottleneck(x)</span><br><span class="line">        out += <span class="variable language_">self</span>.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RestNet50网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet50</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Bottleneck, num_classes=<span class="number">10</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(ResNet50, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.in_channels = <span class="number">64</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = <span class="variable language_">self</span>._make_layer(Bottleneck, <span class="number">64</span>, [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]] * <span class="number">3</span>, [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]] * <span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = <span class="variable language_">self</span>._make_layer(Bottleneck, <span class="number">128</span>, [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]] + [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]] * <span class="number">3</span>, [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]] * <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv4 = <span class="variable language_">self</span>._make_layer(Bottleneck, <span class="number">256</span>, [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]] + [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]] * <span class="number">5</span>, [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]] * <span class="number">6</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv5 = <span class="variable language_">self</span>._make_layer(Bottleneck, <span class="number">512</span>, [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]] + [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]] * <span class="number">2</span>, [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]] * <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, out_channels, strides, paddings</span>):</span><br><span class="line">        layers = []</span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(strides)):</span><br><span class="line">            layers.append(block(<span class="variable language_">self</span>.in_channels, out_channels, strides[i], paddings[i], first=flag))</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            <span class="variable language_">self</span>.in_channels = out_channels * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.conv3(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.conv4(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.conv5(out)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.avgpool(out)</span><br><span class="line">        out = out.reshape(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h2 id="8-DenseNet"><a href="#8-DenseNet" class="headerlink" title="8. DenseNet"></a>8. DenseNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">论文：https://arxiv.org/pdf/1608.06993</span><br><span class="line">https://blog.csdn.net/u014380165/article/details/75142664</span><br><span class="line">而作者则是从feature入手，通过对feature的极致利用达到更好的效果和更少的参数</span><br><span class="line">2017年CVPR，最近一两年卷积神经网络提高效果的方向，要么深（比如ResNet，解决了网络深时候的梯度消失问题）要么宽（比如GoogleNet的Inception），主要还是和ResNet及Inception网络做对比，思想上有借鉴，但却是全新的结构</span><br><span class="line"></span><br><span class="line">先列下DenseNet的几个优点，感受下它的强大：</span><br><span class="line">1、减轻了vanishing-gradient（梯度消失）</span><br><span class="line">2、加强了feature的传递</span><br><span class="line">3、更有效地利用了feature</span><br><span class="line">4、一定程度上较少了参数数量</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110215624.png" alt="截图_20241110215624"></p>
<h2 id="9-ResNeXt"><a href="#9-ResNeXt" class="headerlink" title="9. ResNeXt"></a>9. ResNeXt</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">论文：https://arxiv.org/pdf/1611.05431</span><br><span class="line">博客：</span><br><span class="line">https://blog.csdn.net/zhw864680355/article/details/105419350</span><br><span class="line">https://blog.csdn.net/cp1314971/article/details/104461204</span><br><span class="line"></span><br><span class="line">结合了 ResNet 的残差学习框架和 Inception 的分组卷积思想，旨在在保持模型复杂度不变的情况下提高网络性能。ResNeXt 的主要特征是引入了一种新的“组卷积”的概念，从而将模型的设计转化为简单而有效的分支重复架构。</span><br><span class="line">1. 分组卷积（Group Convolution）：ResNeXt 将一个卷积层拆分为多个“组”，每个组执行独立的卷积操作。这样可以增加模型的“分支”，让网络拥有更强大的表达能力，同时计算代价较低。</span><br><span class="line">2. “Cardinality” 概念：这是 ResNeXt 中引入的一个重要概念，指的是每层中并行分支的数量。增加 cardinality（并行路径数）比增加网络的深度或宽度更能提高模型性能。Cardinality 是 ResNeXt 与 ResNet 之间的重要区别。</span><br><span class="line">3. 简单而模块化的结构：相比于 ResNet，ResNeXt 仅需引入少量的设计修改就能显著提升性能，具有更简单和模块化的设计。</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110212923.png" alt="截图_20241110212923"></p>
<h2 id="10-Res2Net"><a href="#10-Res2Net" class="headerlink" title="10. Res2Net"></a>10. Res2Net</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">论文：https://arxiv.org/pdf/1904.01169</span><br><span class="line">博客：https://blog.csdn.net/u012193416/article/details/121270765</span><br><span class="line">https://blog.csdn.net/qq_40356092/article/details/110077097</span><br><span class="line"></span><br><span class="line">尺度信息或者说局部信息，对象的局部信息对识别对象也很重要，视觉任务想要获得多尺度的表示需要特征提取器使用一个大范围的感受野以不同的尺度描述对象，cnn通过堆叠卷积算子天然能够学习到coarse-to-fine的多尺度特征。这是cnn最重要的一个特性，堆叠cnn扩大感受野，产生了局部信息融合，但是相对于transformer的架构而言，self-attention能够更多的编码全局信息</span><br><span class="line"></span><br><span class="line">作者提出的多尺度并不是layer层级的组合，而是通过在一个更细粒度上产生的多个感受野的组合</span><br><span class="line"></span><br><span class="line">在resnet结构中越到深层，残差激活越小，理论上res2net的结构设计收益应该越小，那么res2net强还是因为其在不是很深的时候带来的特征的深度挖掘能力，多尺度的组合，densenet也是多尺度kernel带来了比resnet更强的收益，模型如果太深，回传的梯度和输入的信息就不一定是最高效的信息。下面这个图也表明res2net其实对于loss所指向的对象的刻画更为准确，就是对干扰梯度的过滤性更好，更细，学的很好，更soft，resnet调整步长让差分更拟合，这个感觉像在步长里面在做小步长，差分里面在差分，soft上soft，解空间更平滑。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110213742.png" alt="截图_20241110213742"></p>
<h2 id="11-SqueezeNet"><a href="#11-SqueezeNet" class="headerlink" title="11. SqueezeNet"></a>11. SqueezeNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">https://zhuanlan.zhihu.com/p/49465950</span><br><span class="line">从LeNet5到DenseNet，反应卷积网络的一个发展方向：提高精度。这里我们开始另外一个方向的介绍：在不大幅降低模型精度的前提下，最大程度的提高运算速度</span><br><span class="line">提高运算所读有两个可以调整的方向：</span><br><span class="line">1.减少可学习参数的数量；</span><br><span class="line">2.减少整个网络的计算量。</span><br><span class="line"></span><br><span class="line">这个方向带来的效果是非常明显的：</span><br><span class="line">1.减少模型训练和测试时候的计算量，单个step的速度更快；</span><br><span class="line">2.减小模型文件的大小，更利于模型的保存和传输；</span><br><span class="line">3.可学习参数更少，网络占用的显存更小</span><br><span class="line"></span><br><span class="line">SqueezeNet是由若干个Fire模块结合卷积网络中卷积层，降采样层，全连接等层组成的。一个Fire模块由Squeeze部分和Expand部分组成</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110220134.png" alt="截图_20241110220134"></p>
<h2 id="12-MobileNet"><a href="#12-MobileNet" class="headerlink" title="12. MobileNet"></a>12. MobileNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PDF:https://arxiv.org/abs/1704.04861</span><br><span class="line">Blogs:https://zhuanlan.zhihu.com/p/31551004</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110220448.png" alt="截图_20241110220448"></p>
<h2 id="13-ShuffleNet"><a href="#13-ShuffleNet" class="headerlink" title="13. ShuffleNet"></a>13. ShuffleNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PDF:https://arxiv.org/pdf/1707.01083</span><br><span class="line">v2 PDF:https://arxiv.org/pdf/1807.11164</span><br><span class="line">zhuanlan.zhihu.com/p/32304419</span><br><span class="line"></span><br><span class="line">ShuffleNet 是专门为计算能力非常有限的移动设备设计的。架构采用了逐点分组卷积和通道shuffle两种新的运算，在保持精度的同时大大降低了计算成本</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110221331.png" alt="截图_20241110221331"></p>
<h2 id="14-SENet"><a href="#14-SENet" class="headerlink" title="14. SENet"></a>14. SENet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PDF:https://arxiv.org/pdf/1709.01507v2</span><br><span class="line">https://zhuanlan.zhihu.com/p/65459972</span><br><span class="line">https://zhuanlan.zhihu.com/p/32702350</span><br><span class="line"> </span><br><span class="line">Squeeze-and-Excitation Networks 简称 SENet,在2017年最后一届 ImageNet 挑战赛(ILSVRC) classification 任务中获得 冠军，将错误率降低到 2.251% </span><br><span class="line">SENet网络的创新点在于关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。</span><br><span class="line">可以把SE模块与其他网络结合。</span><br><span class="line">对于一张图片，不同的channel的权重一般都是不一样的。如果，我们能够把这个信息捕获出来，那么我们的网络就可以获得更多的信息，那么自然就拥有更高得准确率。</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110222136.png" alt="截图_20241110222136"></p>
<h2 id="15-SKNet"><a href="#15-SKNet" class="headerlink" title="15. SKNet"></a>15. SKNet</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PDF：https://arxiv.org/abs/1903.06586</span><br><span class="line"></span><br><span class="line">SKNet类似于SENet，也是一种轻量级、容易集成的有效通道注意力机制。选择内核机制，并不是让模型选择使用哪一个卷积核进行卷积</span><br><span class="line">SKNet的核心在于使用可选择性的多尺度卷积核来捕捉多尺度特征。SKNet引入了一个选择模块，简单理解就是，SKNet先进行不同感受野的卷积得到不同感受野的输出特征图，然后再通过这个选择模块学习这些不同感受野的输出特征图应该占据的权重。</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241110222919.png" alt="截图_20241110222919"></p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PDF：https://arxiv.org/abs/1706.03762</span><br><span class="line">Blogs：</span><br><span class="line">1.How Transformers Work：https://towardsdatascience.com/transformers-141e32e69591?gi=0902b7edd56d</span><br><span class="line">2.详解Transformer：https://zhuanlan.zhihu.com/p/48508221</span><br><span class="line">3.目前主流的attention方法都有哪些：https://www.zhihu.com/question/68482809/answer/264632289</span><br><span class="line">4.为什么Transformer 需要进行Multi-head Attention：https://www.zhihu.com/question/341222779</span><br></pre></td></tr></table></figure>



<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241120231307.png" alt="截图_20241120231307"></p>
<h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://fanrenyi.com/blog/466</span><br></pre></td></tr></table></figure>



<h2 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT(Vision Transformer)"></a>ViT(Vision Transformer)</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PDF：https://arxiv.org/abs/2010.11929</span><br><span class="line">pytorch实现：https://github.com/lucidrains/vit-pytorch</span><br><span class="line">CSDN：https://blog.csdn.net/qq_39478403/article/details/118704747</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241120223253.png" alt="截图_20241120223253"></p>
<h2 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PDF：https://arxiv.org/abs/2103.14030</span><br><span class="line">pytorch实现：https://github.com/microsoft/Swin-Transformer</span><br><span class="line">CSDN：https://blog.csdn.net/qq_37541097/article/details/121119988</span><br><span class="line">知乎：https://zhuanlan.zhihu.com/p/361366090</span><br></pre></td></tr></table></figure>

<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241120213203.png" alt="截图_20241120213203"></p>
<p><img src="/../images/pytorch/%E6%88%AA%E5%9B%BE_20241120213430.png" alt="截图_20241120213430"></p>
<p>Inception v3</p>
<p>ShuffleNet v2</p>
<p>MobileNet v2</p>
<p>MobileNet v3</p>
<p>Wide ResNet</p>
<p>MNASNet</p>
<p>Quantized Models</p>

        </div>

      <div style="background-color: #f9f9f9; border-left: 5px solid #007acc; padding: 10px; margin: 20px 0;">
        <strong>📢 转载须知：</strong>本站点内容采用
    <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 协议</a>进行许可，转载请注明原文链接和作者信息
      </div>


        <!--  -->
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E7%AE%97%E6%B3%95/"># 算法</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span>· </span>
                <a href="/">主页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2025/08/25/ASR%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E5%AD%97/">ASR语音转文字</a>
            
            
            <a class="next" rel="next" href="/2025/08/25/Flask%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/">Flask后端开发</a>
            
        </section>


    </article>
</div>

            </div>
            <!-- <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Henry Li | Powered by Hexo</a> & Chic</a></span>
    </div>
</footer> -->
<!-- <footer id="footer" class="footer">
  <div class="copyright">
    <span>© 2025 Henry Li | Powered by Hexo</a> & Chic</a></span>
    <br>
    <span id="busuanzi_container_site_uv">本站总访客数: <span id="busuanzi_value_site_uv"></span> 人,</span>
    <span id="busuanzi_container_site_pv">本站总访问量: <span id="busuanzi_value_site_pv"></span> 次</span>
    <br>
    <span> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">
      <img alt="CC BY-NC 4.0" style="position:relative; top:2px;" src="https://licensebuttons.net/l/by-nc/4.0/80x15.png" />
      </a>除特别声明外，本站点内容采用<a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
    CC BY-NC-SA 4.0 协议</a>进行许可，转载请注明出处。
<span>
  </div>

  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</footer> -->
<footer id="footer" class="footer" style="font-size:0.3em;line-height:1.5">
  <br>
  <div class="copyright">
    <span>© 2025 Henry Li | Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank" rel="noopener">Chic</a></span>
    <br>
    <span id="busuanzi_container_site_uv">本站总访客数: <span id="busuanzi_value_site_uv"></span> 人,</span>
    <span id="busuanzi_container_site_pv">本站总访问量: <span id="busuanzi_value_site_pv"></span> 次</span>
    <br>
    <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
      <img alt="CC BY-NC-SA 4.0" style="position:relative; top:3px;" src="https://licensebuttons.net/l/by-nc-sa/4.0/80x15.png" />
    </a>本站点内容采用
    <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
      CC BY-NC-SA 4.0 协议</a>进行许可，转载请注明原文链接和作者信息
  </div>

  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</footer>
    </div>
</body>

</html>