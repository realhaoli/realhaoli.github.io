<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Henry Li">





<title>大模型微调 | Henry&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const pagebody = document.getElementsByTagName('body')[0]

            function setTheme(status) {

                if (status === 'dark') {
                    window.sessionStorage.theme = 'dark'
                    pagebody.classList.add('dark-theme');

                } else if (status === 'light') {
                    window.sessionStorage.theme = 'light'
                    pagebody.classList.remove('dark-theme');
                }
            };

            setTheme(window.sessionStorage.theme)
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Henry&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                <a class="menu-item" href="/archives">文章</a>
                
                <a class="menu-item" href="/category">分类</a>
                
                <a class="menu-item" href="/tag">标签</a>
                
                <a class="menu-item" href="/about">关于</a>
                
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Henry&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">
                    <svg class="menu-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M4.5 17.27q-.213 0-.356-.145T4 16.768t.144-.356t.356-.143h15q.213 0 .356.144q.144.144.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.144T4 11.999t.144-.356t.356-.143h15q.213 0 .356.144t.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.143Q4 7.443 4 7.23t.144-.356t.356-.143h15q.213 0 .356.144T20 7.23t-.144.356t-.356.144z"/></svg>
                    <svg class="close-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Material Symbols Light by Google - https://github.com/google/material-design-icons/blob/master/LICENSE --><path fill="currentColor" d="m12 12.708l-5.246 5.246q-.14.14-.344.15t-.364-.15t-.16-.354t.16-.354L11.292 12L6.046 6.754q-.14-.14-.15-.344t.15-.364t.354-.16t.354.16L12 11.292l5.246-5.246q.14-.14.345-.15q.203-.01.363.15t.16.354t-.16.354L12.708 12l5.246 5.246q.14.14.15.345q.01.203-.15.363t-.354.16t-.354-.16z"/></svg>
                </div>
            </div>
            <div class="menu" id="mobile-menu">
                
                <a class="menu-item" href="/archives">文章</a>
                
                <a class="menu-item" href="/category">分类</a>
                
                <a class="menu-item" href="/tag">标签</a>
                
                <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.classList.contains("active")) {
            toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        } else {
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">关闭目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">前往底部</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 6
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        // b.innerText = expanded ? 'Expand all' : 'Collapse all';
        b.innerText = expanded ? '展开目录' : '关闭目录';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">大模型微调</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Henry Li</a>&nbsp;&nbsp;
                    

                    
                        <span class="post-time">
                        Date: <a href="#">2025-08-28</a>&nbsp;&nbsp;
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E6%8A%80%E6%9C%AF%E9%80%9F%E8%AE%B0/">技术速记</a>
                            &nbsp;&nbsp;
                        </span>
                    
                    <!-- 添加本文阅读量 -->
                        <span id="busuanzi_container_page_pv">
                        本文阅读量: <span id="busuanzi_value_page_pv"></span> 次
                        </span>

                </div>
            
        </header>

        <div class="post-content">
            <h1 id="1-大模型微调概念"><a href="#1-大模型微调概念" class="headerlink" title="1. 大模型微调概念"></a>1. 大模型微调概念</h1><p>本笔记根据学习课程整理，视频链接：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1YLE1zyEvX/">https://www.bilibili.com/video/BV1YLE1zyEvX/</a></p>
<h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><p>在已有大规模预训练模型基础上，通过对标注数据进行训练，以适应特定任务或场景的需求。</p>
<p>微调是通过修改模型参数来优化模型能力，是一种能够让模型“永久”掌握某种能力的方法</p>
<ol>
<li><strong>全量微调</strong>：预训练权重全部放开，算力消耗更大，改造更为彻底。处理不当，很可能造成模型原始能力的灾难性遗忘</li>
<li><strong>高效微调</strong>：只带入部分数据进行微调，修改模型部分参数，来调整模型整体能力</li>
<li><strong>零&#x2F;少样本提示工程</strong>：不改任何参数，靠精心构造的 prompt + 少量示例完成任务</li>
</ol>
<p><strong>高效微调应用场景</strong></p>
<ul>
<li><strong>对话风格微调</strong>：针对客服系统、虚拟助理等场景，通过微调来适应不同的语气、礼貌程度或回答方式</li>
<li><strong>知识灌注</strong>：指将外部知识或领域特定的信息快速集成到已有的预训练模型中，使用少量的标注数据对预训练模型进行微调，帮助模型理解特定行业的术语</li>
<li><strong>推理能力提升</strong>：通过微调，模型能够更加高效地理解长文本、推理隐含信息，或者从数据中提取逻辑关系</li>
<li><strong>Agent能力（Function calling能力、或者MCP能力）提升</strong>：多任务协作或功能调用场景中，有效地与其他系统进行交互、调用外部API或执行特定任务</li>
</ul>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心思想</th>
<th>需训参数占比</th>
<th>典型显存节省</th>
<th>特点&#x2F;坑</th>
</tr>
</thead>
<tbody><tr>
<td>LoRA</td>
<td>在原权重旁路加低秩分解 ΔW&#x3D;BA，冻结原W</td>
<td>0.1 %~1 %</td>
<td>30 %~50 %</td>
<td>实现简单，可合并推理；秩r选大了反而掉点</td>
</tr>
<tr>
<td>AdaLoRA&#x2F;QLoRA</td>
<td>LoRA+动态秩+4-bit量化</td>
<td>&lt;0.3 %</td>
<td>70 %</td>
<td>单卡48 G可训65 B模型；推理需反量化</td>
</tr>
<tr>
<td>Adapter</td>
<td>在Transformer每层后加2层MLP瓶颈</td>
<td>2 %~5 %</td>
<td>40 %</td>
<td>任务间切换只需换Adapter；推理延迟略增</td>
</tr>
<tr>
<td>Prefix Tuning</td>
<td>在K&#x2F;V前拼接可训练前缀向量</td>
<td>0.1 %</td>
<td>30 %</td>
<td>生成任务可控性强；前缀长度需调</td>
</tr>
<tr>
<td>P-Tuning v2</td>
<td>把提示token插到每层hidden states</td>
<td>0.3 %</td>
<td>35 %</td>
<td>小样本表现好；实现稍复杂</td>
</tr>
<tr>
<td>Prompt Tuning</td>
<td>仅在输入层拼接软提示</td>
<td>&lt;0.01 %</td>
<td>20 %</td>
<td>几乎不占显存；但对难任务精度差</td>
</tr>
</tbody></table>
<p>大模型微调不是非全参不可，先 Prompt，再 LoRA，实在不行才全参”——按算力和数据量逐级升配即可  </p>
<h2 id="1-2-LoRA、QLoRA"><a href="#1-2-LoRA、QLoRA" class="headerlink" title="1.2 LoRA、QLoRA"></a>1.2 LoRA、QLoRA</h2><ol>
<li><strong>LoRA</strong>通过引入低秩矩阵来减少微调时需要调整的参数数量，从而显著降低显存和计算资源的消耗。</li>
</ol>
<p>不直接调整原始模型的所有参数，而是通过在某些层中插入低秩的适配器（Adapter）层来进行训练。</p>
<ol>
<li><strong>QLoRA</strong>结合了 LoRA 的<strong>低秩适配器</strong>和<strong>量化技术</strong></li>
</ol>
<p><strong>量化技术</strong>：通过将模型权重量化为低精度（如 INT4），减少内存占用，并提高推理和训练速度</p>
<table>
<thead>
<tr>
<th></th>
<th>LoRA</th>
<th>QLoRA</th>
</tr>
</thead>
<tbody><tr>
<td>核心技术</td>
<td>低秩适配器（Low-RankAdapters）</td>
<td>低秩适配器 + 量化技术（Low-Rank Adapters + Quantization）</td>
</tr>
<tr>
<td>适用场景</td>
<td>显存受限，但设备性能较好</td>
<td>极限显存受限或需要快速推理的设备</td>
</tr>
<tr>
<td>计算效率</td>
<td>提高计算效率，减少调整的参数数量</td>
<td>进一步提升效率，减少内存使用并加快推理速度</td>
</tr>
<tr>
<td>量化技术</td>
<td>无量化</td>
<td>将权重量化为低精度（如 INT4 或 INT8）</td>
</tr>
<tr>
<td>训练复杂度</td>
<td>较简单，适用于大多数微调场</td>
<td>需要更多的量化和适配工作，但适合超大模型和设备受限场景</td>
</tr>
</tbody></table>
<h2 id="1-3-主流微调工具"><a href="#1-3-主流微调工具" class="headerlink" title="1.3 主流微调工具"></a>1.3 主流微调工具</h2><ol>
<li><p>较高封装的微调四套工具：unsloth、Llama-Factory、ms-SWIFT和ColossalAI</p>
</li>
<li><p>更底层的方法或使用更加底层的库，如peft、LoRA、transformer</p>
</li>
</ol>
<h3 id="（1）unsloth"><a href="#（1）unsloth" class="headerlink" title="（1）unsloth"></a>（1）unsloth</h3><p>动态量化可以极大程度压缩模型运行所需占用的显存大小，同时几乎不损失性能，但问题是<strong>只能单卡运行</strong>，其吞吐量有限</p>
<ul>
<li>unsloth 是一个专为大型语言模型（LLM）设计的动态量化与微调框架，旨在提高微调效率并减少显存占用。</li>
<li><strong>高效微调</strong>： unsloth 通过深度优化，使 LLM 的微调速度提高 2-5 倍，显存使用量减少约 80%，且准确度无明显下降。</li>
<li>unsloth 与 HuggingFace <strong>生态兼容</strong>，可以很容易地transformers、peft、trl 等库结合，以实现模型的监督微调（SFT）和直接偏好优化（DPO），仅需模型的加载方式，无需对现有训练代码进行修改。</li>
<li><strong>内存优化</strong>： 通过 4 位和 16 位的 QLoRA&#x2F;LoRA 微调，unsloth 显著了显存占用，使得在资源受限的环境中也能大的微调。</li>
</ul>
<h3 id="（2）LLama-Factory"><a href="#（2）LLama-Factory" class="headerlink" title="（2）LLama-Factory"></a>（2）LLama-Factory</h3><p><strong>单机多卡</strong></p>
<ul>
<li><p><strong>广型支持</strong>： LLaMA-Factory 支持对 100 多LLMs 和 VLMs 进行微调</p>
</li>
<li><p><strong>高效的微调方法</strong>： 框架集成了多nk Adaptation、QRA（Quantized LoRA）等，以提高训练速度</p>
</li>
</ul>
<p>并减少显存占用。</p>
<ul>
<li><p><strong>多模态任务支持</strong>：文本任务外，音频识别、音频理解等多种任务类型。</p>
</li>
<li><p><strong>实验监控</strong>： 提供了丰富的实验监控工具，如 LlamaBoard、TensorBoard、Wandb、MLflow</p>
</li>
<li><p>框架提供了类似 OpenAI 风格的 API、Gradio UI 和命令行界面，并结合 vLLM worker，实现了高效的推理能力。</p>
</li>
</ul>
<h3 id="（3）ms-SWIFT"><a href="#（3）ms-SWIFT" class="headerlink" title="（3）ms-SWIFT"></a>（3）ms-SWIFT</h3><p>由魔搭社区（ModelScope）开发的高效微调和部署框架，支持<strong>单机多卡</strong>，旨在为研究人员和开发者提供一站式的大模型与多模态大模型的训练、推理、评测、量化和部署解决方案</p>
<ul>
<li><p><strong>多样化的训练技术</strong>： 框架集oRA、Llama-Pro、LonoRA、GaLore、Q-GaLore、LoRA+、LISA、DoRA、FourierFt、ReFT、UnSloth 和 Liger 等，满足不同的微调需求。</p>
</li>
<li><p><strong>轻量级微调</strong>： 支持多种轻量级微调方法，如 LoRA、QLoRA、DoLLaMAPro、Adapt、GaLore、Q-Galore、LISA、UnSloth、Liger-Kernel 等，降低显存和计算资源的消耗。</p>
</li>
<li><p><strong>分布式训练</strong>： 支持分布式数据并行（DDP）、DeepSpeed ZeRO2&#x2F;ZeRO3、FSDP 等技术，提升推理加速：提供 BNBWQ、GPTQ、AQLM、HQQ、EETQ 等量化方法，并支持使用 vLLM 和LMDeploy 对推理、评测和部署 支持图像、视频和语音等多种模态型训练，涵盖 VQA、Caption、OCR、Grounding 等任务。</p>
</li>
<li><p><strong>用户友好的界面</strong>： 提供基于 Gradio 的 We和量化操作，简化了大模型的全链路流程。</p>
</li>
</ul>
<h3 id="（4）ColossalAI"><a href="#（4）ColossalAI" class="headerlink" title="（4）ColossalAI"></a>（4）ColossalAI</h3><p>Colossal-AI 提供了自动超高维并行、大规模优化库、自适应任务调度、内存优化以及最新模型复现等前沿技术。</p>
<p><strong>多机多卡，工业级</strong></p>
<h2 id="1-4-性能评估框架EvalScope"><a href="#1-4-性能评估框架EvalScope" class="headerlink" title="1.4 性能评估框架EvalScope"></a>1.4 性能评估框架EvalScope</h2><p>由阿里巴巴魔搭社区（ModelScope）推出的一款<strong>开源模型评估框架</strong>，旨在为大语言模型（LLM）和多模态模型提供统一、系统化的性能评估方案。</p>
<ul>
<li><p><strong>丰富的评测基准覆盖</strong>：框架内置多种权威评测数据集，涵盖中英文通用知识问答（如 MMLU、CMMLU、C-Eval）、数学推理（如 GSM8K、MATH）、常识判断（如 HellaSwag、ARC）、代码生成（如 HumanEval）等多个方向，支持对模型能力进行多维度评估。</p>
</li>
<li><p><strong>多样的评估模式支持</strong>：EvalScope 提供三种灵活的评估模式，包括单模型评估模式（Single）、基于基线的两两对比模式（Pairwise-Baseline）、以及全模型两两对比模式（Pairwise-All），可满足从快速诊断到全面对比的不同使用场景。</p>
</li>
<li><p><strong>统一的模型接入接口</strong>：框架对不同类型的模型提供统一的调用方式，兼容 HuggingFace、本地部署模型及 API 远程调用，支持标准的 generate 与 chat 接口，大大降低了模型集成的复杂度。</p>
</li>
<li><p><strong>评估流程高度自动化</strong>：EvalScope 实现了评测任务的全自动执行，包括客观题自动打分、复杂问题使用评审模型辅助判定结果等，支持批量评估与日志记录，极大提升了评估效率与结果一致性。</p>
</li>
<li><p><strong>完善的性能与能力可视化工具</strong>：框架支持生成详细的评估报告和图表，展示模型在不同任务维度下的表现，便于开发者进行横向对比和性能分析。</p>
</li>
<li><p><strong>多后端与评测能力扩展</strong>：EvalScope 可集成多个评测后端，如 OpenCompass、VLMEvalKit、RAGEval 等，支持从单模态到多模态、从语言建模到 RAG 端到端评测的全链路能力。</p>
</li>
<li><p><strong>支持部署性能测试</strong>：除评估模型能力外，EvalScope 还提供服务端推理性能测试工具，涵盖吞吐量、响应时延等关键指标，帮助开发者评估模型的部署实用性。</p>
</li>
</ul>
<h1 id="2-微调步骤"><a href="#2-微调步骤" class="headerlink" title="2. 微调步骤"></a>2. 微调步骤</h1><h2 id="2-1-部署环境"><a href="#2-1-部署环境" class="headerlink" title="2.1 部署环境"></a>2.1 部署环境</h2><h3 id="（1）环境要求"><a href="#（1）环境要求" class="headerlink" title="（1）环境要求"></a>（1）环境要求</h3><p>需要考虑硬件方面，不同尺寸模型、不同精度微调时所需显存</p>
<p>CPU不能进行微调；目前MoE模型只支持4bit普通量化微调，暂不支持动态量化微调。</p>
<p><strong>注：下面操作部分我使用4090显卡，微调Qwen3-8B模型进行训练，使用的工具如下</strong></p>
<ul>
<li><p>【必须】Unsloth：高效微调框架，必须安装使用；</p>
</li>
<li><p>【可选】vLLM：模型调度框架，用于验证微调后模型效果，也可以使用ollama或者其他调度框架</p>
</li>
</ul>
<p>进行模型微调后效果验证；</p>
<ul>
<li><p>【可选】EvalScope：模型评测框架，用于对比微调前后模型性能，也可以通过人工观察进行评估；</p>
</li>
<li><p>【可选】wandb：模型训练数据在线记录工具，用于保存模型训练过程中损失之的变化情况，并监</p>
</li>
</ul>
<p>控服务器硬件数据；</p>
<p>注：以上四个可能会发生环境冲突，所以最好放入不同的conda环境里</p>
<h3 id="（2）安装Unsloth"><a href="#（2）安装Unsloth" class="headerlink" title="（2）安装Unsloth"></a>（2）安装Unsloth</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装Unsloth的conda环境</span></span><br><span class="line">conda create --name unsloth python=3.11</span><br><span class="line">conda init</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment">#进入环境</span></span><br><span class="line">conda activate unsloth</span><br><span class="line"><span class="comment">#安装Jupyter及Jupyter Kerne</span></span><br><span class="line">conda install jupyterlab</span><br><span class="line">conda install ipykernel</span><br><span class="line">python -m ipykernel install --user --name unsloth --display-name <span class="string">&quot;Python unsloth&quot;</span></span><br><span class="line"><span class="comment">#安装Unsloth</span></span><br><span class="line">pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试安装是否成功</span></span><br><span class="line">from unsloth import FastLanguageModel</span><br><span class="line">import torch</span><br></pre></td></tr></table></figure>

<h3 id="（3）安装vLLM"><a href="#（3）安装vLLM" class="headerlink" title="（3）安装vLLM"></a>（3）安装vLLM</h3><p>vllm主要提供后台的模型调用服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装vLLM的conda环境</span></span><br><span class="line">conda create --name vllm python=3.11</span><br><span class="line">conda init</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line">conda activate vllm</span><br><span class="line"><span class="comment">#安装vLLM</span></span><br><span class="line">pip install bitsandbytes&gt;=0.45.3</span><br><span class="line">pip install --upgrade vllm</span><br></pre></td></tr></table></figure>

<h3 id="（4）Qwen3模型权重下载"><a href="#（4）Qwen3模型权重下载" class="headerlink" title="（4）Qwen3模型权重下载"></a>（4）Qwen3模型权重下载</h3><p>魔搭社区选择模型：<a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/">https://www.modelscope.cn/models/</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope</span><br><span class="line">modelscope download --model Qwen/Qwen3-8B --local_dir ./Qwen/Qwen3-8B</span><br></pre></td></tr></table></figure>

<h3 id="（5）vllm模型调用"><a href="#（5）vllm模型调用" class="headerlink" title="（5）vllm模型调用"></a>（5）vllm模型调用</h3><p>用 vLLM 把本地的 Qwen&#x2F;Qwen3-8B 模型启动成一个兼容 OpenAI 格式的在线推理服务，并开启“工具调用（function-calling）”能力</p>
<p>注意：Unsloth的动态量化模型只支持单卡调用，最低22G显存即可运行。因此这里是设置的单卡运行vLLM</p>
<p>vLLM调用qwen3参数说明:<a target="_blank" rel="noopener" href="https://qwen.readthedocs.io/en/latest/deployment/vllm.html#">https://qwen.readthedocs.io/en/latest/deployment/vllm.html#</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /root/autodl-tmp/Qwen，运行大模型</span></span><br><span class="line">vllm serve ./Qwen3-8B --enable-auto-tool-choice --tool-call-parser hermes</span><br><span class="line"><span class="comment">#如果报错说明GPU显存不够，以下添加参赛设置为32000（默认40960）</span></span><br><span class="line"><span class="comment">#vllm serve ./Qwen3-8B --max-model-len 32000 --enable-auto-tool-choice --tool-call-parser hermes</span></span><br></pre></td></tr></table></figure>

<p><img src="/../images/llm-finetuning/27.png" alt="截屏2025-08-28 17.27.23"></p>
<p>可用以下代码测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ./Qwen/test.py</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    api_key=<span class="string">&quot;EMPTY&quot;</span>,   <span class="comment"># vLLM 不要求真正的 key</span></span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8000/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你好，好久不见！&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    model=<span class="string">&quot;./Qwen3-8B&quot;</span>, </span><br><span class="line">    messages=messages</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure>

<p><img src="/../images/llm-finetuning/35.png" alt="截屏2025-08-28 17.35.51"></p>
<h3 id="（6）安装EvalScope"><a href="#（6）安装EvalScope" class="headerlink" title="（6）安装EvalScope"></a>（6）安装EvalScope</h3><p>需要调用vllm进行测试，官网：<a target="_blank" rel="noopener" href="https://github.com/modelscope/evalscope">https://github.com/modelscope/evalscope</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建evalscope的conda环境</span></span><br><span class="line">conda create --name evalscope python=3.11</span><br><span class="line">conda init</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line">conda activate evalscope</span><br><span class="line"><span class="comment">#如果实验jupyter则需要运行</span></span><br><span class="line">conda install jupyterlab</span><br><span class="line">conda install ipykernel</span><br><span class="line">python -m ipykernel install --user --name evalscope --display-name <span class="string">&quot;Pythonevalscope&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#然后安装需要的库</span></span><br><span class="line">pip install evalscope <span class="comment"># 安装 Native backend (默认)</span></span><br><span class="line"><span class="comment"># 额外选项</span></span><br><span class="line">pip install <span class="string">&#x27;evalscope[opencompass]&#x27;</span> 	<span class="comment"># 安装 OpenCompass backend</span></span><br><span class="line">pip install <span class="string">&#x27;evalscope[vlmeval]&#x27;</span> 			<span class="comment"># 安装 VLMEvalKit backend</span></span><br><span class="line">pip install <span class="string">&#x27;evalscope[rag]&#x27;</span> 					<span class="comment"># 安装 RAGEval backend</span></span><br><span class="line">pip install <span class="string">&#x27;evalscope[perf]&#x27;</span> 				<span class="comment"># 安装 模型压测模块 依赖</span></span><br><span class="line">pip install <span class="string">&#x27;evalscope[app]&#x27;</span> 					<span class="comment"># 安装 可视化 相关依赖</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或可以直接输入all，安装全部模块</span></span><br><span class="line"><span class="comment"># pip install &#x27;evalscope[all]&#x27; # 安装所有 backends (Native, OpenCompass, VLMEvalKit, RAGEval)</span></span><br></pre></td></tr></table></figure>

<h3 id="（7）EvalScope压力测试"><a href="#（7）EvalScope压力测试" class="headerlink" title="（7）EvalScope压力测试"></a>（7）EvalScope压力测试</h3><p>测试Qwen3-8B模型在4090显卡上，由vllm调度框架驱动时的实际性能表现：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">evalscope perf \</span><br><span class="line">	--url <span class="string">&quot;http://127.0.0.1:8000/v1/chat/completions&quot;</span> \</span><br><span class="line">	--parallel 5 \</span><br><span class="line">	--model ./Qwen3-8B \</span><br><span class="line">	--number 20 \</span><br><span class="line">	--api openai \</span><br><span class="line">	--dataset openqa \</span><br><span class="line">	--stream</span><br></pre></td></tr></table></figure>

<p><img src="/../images/llm-finetuning/01.png" alt="截屏2025-08-28 18.01.49"></p>
<p>分析结果文件保存在：outputs&#x2F;20250826_175659&#x2F;Qwen3-8B，可在summary.json中查看压测的benchmark</p>
<h3 id="（8）EvalScope性能测试"><a href="#（8）EvalScope性能测试" class="headerlink" title="（8）EvalScope性能测试"></a>（8）EvalScope性能测试</h3><p>首先测试模型性能，然后微调大模型，微调后再次测试大模型性能</p>
<p>权威的全面的构建微调大模型的数据集(需要微调好多天)如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> evalscope.collections <span class="keyword">import</span> CollectionSchema, DatasetInfo, WeightedSampler</span><br><span class="line"><span class="keyword">from</span> evalscope.utils.io_utils <span class="keyword">import</span> dump_jsonl_data</span><br><span class="line"></span><br><span class="line">schema = CollectionSchema(name=<span class="string">&#x27;Qwen3&#x27;</span>, datasets=[</span><br><span class="line">    CollectionSchema(name=<span class="string">&#x27;English&#x27;</span>, datasets=[</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;mmlu_pro&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;exam&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;mmlu_redux&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;exam&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;ifeval&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;instruction&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">    ]),</span><br><span class="line">    CollectionSchema(name=<span class="string">&#x27;Chinese&#x27;</span>, datasets=[</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;ceval&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;exam&#x27;</span>, tags=[<span class="string">&#x27;zh&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;iquiz&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;exam&#x27;</span>, tags=[<span class="string">&#x27;zh&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">    ]),</span><br><span class="line">    CollectionSchema(name=<span class="string">&#x27;Code&#x27;</span>, datasets=[</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;live_code_bench&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;code&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;</span><br><span class="line">            <span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">&#x27;subset_list&#x27;</span>: [<span class="string">&#x27;v5_v6&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;extra_params&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;start_date&#x27;</span>: <span class="string">&#x27;2025-01-01&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;end_date&#x27;</span>: <span class="string">&#x27;2025-04-30&#x27;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;),</span><br><span class="line">    ]),</span><br><span class="line">    CollectionSchema(name=<span class="string">&#x27;Math&amp;Science&#x27;</span>, datasets=[</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;math_500&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;math&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;aime24&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;math&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;aime25&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;math&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;<span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span>&#125;),</span><br><span class="line">        DatasetInfo(name=<span class="string">&#x27;gpqa&#x27;</span>, weight=<span class="number">1</span>, task_type=<span class="string">&#x27;knowledge&#x27;</span>, tags=[<span class="string">&#x27;en&#x27;</span>], args=&#123;</span><br><span class="line">            <span class="string">&#x27;subset_list&#x27;</span>: [<span class="string">&#x27;gpqa_diamond&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;few_shot_num&#x27;</span>: <span class="number">0</span></span><br><span class="line">        &#125;),</span><br><span class="line">    ])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取混合数据</span></span><br><span class="line">mixed_data = WeightedSampler(schema).sample(<span class="number">100000000</span>)  <span class="comment"># 设置一个很大的数字以确保所有数据集都被采样</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将混合数据导出到 JSONL 文件</span></span><br><span class="line">dump_jsonl_data(mixed_data, <span class="string">&#x27;outputs/qwen3_test.jsonl&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>也可使用<code>EvalScope</code>专门为<code>Qwen3</code>准备的 <code>modelscope/EvalScope-Qwen3-Test</code>数据集进行评测，会围绕模型的推理、指令跟随、代理能力和多语言支持方面能力进行测试，该数据包含 <code>mmlu_pro</code>、<code>ifeval</code>、 <code>live_code_bench</code>、 <code>math_500</code>、 <code>aime24</code>等各著名评估数据集。</p>
<p>测试代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> evalscope <span class="keyword">import</span> TaskConfig, run_task</span><br><span class="line"></span><br><span class="line">task_cfg = TaskConfig(</span><br><span class="line">    model=<span class="string">&#x27;./Qwen3-8B&#x27;</span>,</span><br><span class="line">    api_url=<span class="string">&#x27;http://127.0.0.1:8000/v1/chat/completions&#x27;</span>,</span><br><span class="line">    eval_type=<span class="string">&#x27;loacl&#x27;</span>,          <span class="comment"># 或service，openai_api</span></span><br><span class="line">    datasets=[<span class="string">&#x27;data_collection&#x27;</span>],</span><br><span class="line">    dataset_args=&#123;</span><br><span class="line">        <span class="string">&#x27;data_collection&#x27;</span>: &#123;</span><br><span class="line">            <span class="comment"># &#x27;dataset_id&#x27;: &#x27;modelscope/EvalScope-Qwen3-Test&#x27;,</span></span><br><span class="line">            <span class="string">&#x27;dataset_id&#x27;</span>: <span class="string">&#x27;./datasets/qwen3-test/qwen3_test.jsonl&#x27;</span>, <span class="comment">#本地数据</span></span><br><span class="line">            <span class="string">&#x27;subset&#x27;</span>: <span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;filters&#x27;</span>: &#123;<span class="string">&#x27;remove_until&#x27;</span>: <span class="string">&#x27;&lt;/think&gt;&#x27;</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    eval_batch_size=<span class="number">128</span>,</span><br><span class="line">    generation_config=&#123;</span><br><span class="line">        <span class="string">&#x27;max_tokens&#x27;</span>: <span class="number">30000</span>,<span class="comment">#最大生成token数，过小会被截断</span></span><br><span class="line">        <span class="string">&#x27;temperature&#x27;</span>: <span class="number">0.6</span>, <span class="comment">#采样温度</span></span><br><span class="line">        <span class="string">&#x27;top_p&#x27;</span>: <span class="number">0.95</span>,      <span class="comment">#top-p采样</span></span><br><span class="line">        <span class="string">&#x27;top_k&#x27;</span>: <span class="number">20</span>,        <span class="comment">#top-k采样</span></span><br><span class="line">        <span class="string">&#x27;n&#x27;</span>: <span class="number">1</span>              <span class="comment">#每个请求产生的回复量</span></span><br><span class="line">    &#125;,</span><br><span class="line">    timeout=<span class="number">60000</span>,          <span class="comment">#超时时间</span></span><br><span class="line">    stream=<span class="literal">True</span>,            <span class="comment">#是否流式输出</span></span><br><span class="line">    limit=<span class="number">2000</span>,             <span class="comment">#使用多少条数据测试</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">run_task(task_cfg=task_cfg)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行</span></span><br><span class="line">python eval_api.py</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型能力评测结果，可视化</span></span><br><span class="line">evalscope app</span><br></pre></td></tr></table></figure>

<h3 id="（9）安装wandb"><a href="#（9）安装wandb" class="headerlink" title="（9）安装wandb"></a>（9）安装wandb</h3><p>接下来在unsloth微调前，我们即可设置wandb进行微调记录，</p>
<p>模型训练过程中随时记录硬件性能、损失值、学习率等信息传到他的官网，可视化监控</p>
<ol>
<li><p>wandb官网注册：<a target="_blank" rel="noopener" href="https://wandb.ai/site">https://wandb.ai/site</a></p>
</li>
<li><p>记录APIkey</p>
</li>
<li><p>安装wandb包</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wandb</span><br></pre></td></tr></table></figure>

<h1 id="2-大模型微调"><a href="#2-大模型微调" class="headerlink" title="2. 大模型微调"></a>2. 大模型微调</h1><h2 id="2-1-数据集"><a href="#2-1-数据集" class="headerlink" title="2.1 数据集"></a>2.1 数据集</h2><h3 id="（1）下载数据集"><a href="#（1）下载数据集" class="headerlink" title="（1）下载数据集"></a>（1）下载数据集</h3><p>&emsp;&emsp;若要选择公开数据集进行高效微调，则首先可以考虑HuggingFace和ModelScope平台上的数据集。其中HuggingFace上不仅保管了最大规模数量的数据集，而且HuggingFace的dataset工具，也是目前主流微调核心库如trl库默认支持的数据集格式。此外，ModelScope则是国内版的“HuggingFace”，拥有最大规模的中文数据集。</p>
<p>围绕Qwen3模型的高效微调，Qwen3 具备推理模式和非推理模式，为了确保其仍然保留混合推理能力，围绕普通对话数据集FineTome、带有推理字段的数学类数据集OpenMathReasoning这<strong>两个数据集进行拼接</strong>，不断调整COT数学数据集和普通文本问答数据集之间的配比，从而在确保能提升模型的数学能力的同时，保留非推理的功能。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实际微调过程中，大多都会使用huggingface的datasets库进行数据集下载和管理，实际下载流程如下</span></span><br><span class="line">pip install --upgrade datasets huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 HTTP 和 HTTPS 代理</span></span><br><span class="line">os.environ[<span class="string">&quot;HTTP_PROXY&quot;</span>] = <span class="string">&quot;http://127.0.0.1:10080&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;HTTPS_PROXY&quot;</span>] = <span class="string">&quot;http://127.0.0.1:10080&quot;</span></span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;<code>datasets</code> 是 Hugging Face 提供的一个高效数据处理库，专为机器学习和大语言模型（LLM）训练而设计。它支持加载、处理、转换和保存各种格式的数据（如 JSON、CSV、Parquet 等），并能与 <code>transformers</code> 模型无缝集成。通过 <code>datasets</code>，开发者可以快速完成数据清洗、切分、tokenization 等常见任务，大大提升训练效率，特别适合用于指令微调、对话生成、Function Calling 等任务的数据预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别下载并导入</span></span><br><span class="line">reasoning_dataset = load_dataset(<span class="string">&quot;unsloth/OpenMathReasoning-mini&quot;</span>, split = <span class="string">&quot;cot&quot;</span>) <span class="comment"># 只下载包含cot的数据集</span></span><br><span class="line">non_reasoning_dataset = load_dataset(<span class="string">&quot;mlabonne/FineTome-100k&quot;</span>, split = <span class="string">&quot;train&quot;</span>) <span class="comment">#只下载train部分数据</span></span><br><span class="line"><span class="comment">#print(reasoning_dataset) #查看数据集基本信息</span></span><br><span class="line"><span class="comment">#print(reasoning_dataset[0])#查看指定数据集基本信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(reasoning_dataset))<span class="comment">#查看数据长度</span></span><br></pre></td></tr></table></figure>

<h3 id="（2）数据集清洗"><a href="#（2）数据集清洗" class="headerlink" title="（2）数据集清洗"></a>（2）数据集清洗</h3><p>主要是围绕数据集进行数据格式的调整，便于后续带入Qwen3提示词模板。</p>
<ol>
<li><strong>dataset格式的数据集</strong></li>
</ol>
<p>可以先创建满足格式调整的函数，然后<strong>使用map方法对数据集格式进行调整</strong>。这里先创建generate_conversation函数，用于对reasoning_dataset中的每一条数据进行格式调整，即通过新创建一个新的特征conversations，来以对话形式保存历史问答数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_conversation</span>(<span class="params">examples</span>):</span><br><span class="line">    problems  = examples[<span class="string">&quot;problem&quot;</span>]</span><br><span class="line">    solutions = examples[<span class="string">&quot;generated_solution&quot;</span>]</span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> problem, solution <span class="keyword">in</span> <span class="built_in">zip</span>(problems, solutions):</span><br><span class="line">        conversations.append([</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;user&quot;</span>,      <span class="string">&quot;content&quot;</span> : problem&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span> : solution&#125;,</span><br><span class="line">        ])</span><br><span class="line">    <span class="keyword">return</span> &#123; <span class="string">&quot;conversations&quot;</span>: conversations, &#125;</span><br><span class="line"><span class="comment">#print(reasoning_dataset[0])</span></span><br><span class="line">reasoning_data = reasoning_dataset.<span class="built_in">map</span>(generate_conversation, batched = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#print(reasoning_data[&quot;conversations&quot;])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将其带入Qwen3的提示词模板中进行转化</span></span><br><span class="line">reasoning_conversations = tokenizer.apply_chat_template(</span><br><span class="line">    reasoning_data[<span class="string">&quot;conversations&quot;</span>],</span><br><span class="line">    tokenize = <span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment">#print(reasoning_conversations[0])</span></span><br><span class="line"><span class="comment">#&quot;&lt;|im_start|&gt;user ...&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\....</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>sharegpt对话格式数据集</strong></li>
</ol>
<p>non_reasoning_conversations数据集，因此可以直接借助Unsloth的standardize_sharegpt库进行数据集的格式转化，转化效果如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth.chat_templates <span class="keyword">import</span> standardize_sharegpt</span><br><span class="line"></span><br><span class="line">dataset = standardize_sharegpt(non_reasoning_dataset)</span><br><span class="line"><span class="comment"># print(dataset[&quot;conversations&quot;][0])</span></span><br><span class="line"><span class="comment"># 可直接带入Qwen3对话模板中进行格式调整</span></span><br><span class="line">non_reasoning_conversations = tokenizer.apply_chat_template(</span><br><span class="line">    dataset[<span class="string">&quot;conversations&quot;</span>],</span><br><span class="line">    tokenize = <span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># print(non_reasoning_conversations[0])</span></span><br><span class="line"><span class="string">&#x27;&lt;|im_start|&gt;user\nExplain what boolean operators are,...&lt;|im_end|&gt;...</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>按比例抽取</strong></li>
</ol>
<p>这两个数据集并不均衡，能看得出非推理类数据集的长度更长。我们假设希望模型保留一定的推理能力，但又特别希望它作为一个聊天模型来使用。因此，我们需要定义一个仅聊天数据的比例。目标是从两个数据集中构建一个混合训练集。</p>
<p>这里我们可以设定一个 25% 推理数据、75% 聊天数据的比例，最后将这两个数据集合并起来即可。这里我们需要先将上述list格式的数据转化为pd.Series数据，然后进行采样，并最终将其转化为dataset类型对象。（此外也可以先转化为dataset对象类型，然后再进行采样）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">chat_percentage = <span class="number">0.75</span></span><br><span class="line"></span><br><span class="line">non_reasoning_subset = pd.Series(non_reasoning_conversations)</span><br><span class="line">non_reasoning_subset = non_reasoning_subset.sample(</span><br><span class="line">    <span class="built_in">int</span>(<span class="built_in">len</span>(reasoning_conversations) * (<span class="number">1.0</span> - chat_percentage)),</span><br><span class="line">    random_state = <span class="number">2407</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 然后进行拼接和转化</span></span><br><span class="line">data = pd.concat([</span><br><span class="line">    pd.Series(reasoning_conversations),</span><br><span class="line">    pd.Series(non_reasoning_subset)</span><br><span class="line">])</span><br><span class="line">data.name = <span class="string">&quot;text&quot;</span></span><br><span class="line"></span><br><span class="line">combined_dataset = Dataset.from_pandas(pd.DataFrame(data))</span><br><span class="line">combined_dataset = combined_dataset.shuffle(seed = <span class="number">3407</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据集保存</span></span><br><span class="line">combined_dataset.save_to_disk(<span class="string">&quot;cleaned_qwen3_dataset&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后续使用时即可使用如下代码进行读取</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_from_disk</span><br><span class="line">combined_dataset = load_from_disk(<span class="string">&quot;cleaned_qwen3_dataset&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="2-2-Unsloth使用"><a href="#2-2-Unsloth使用" class="headerlink" title="2.2 Unsloth使用"></a>2.2 Unsloth使用</h2><p>Unsloth是一个集模型调用和高效微调为一体的框架，在开始进行模型微调前，可以先尝试借助Unsloth进行模型调用</p>
<h3 id="（1）基本使用方式"><a href="#（1）基本使用方式" class="headerlink" title="（1）基本使用方式"></a>（1）基本使用方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#0. 指定显卡</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;1&quot;</span>	<span class="comment">#Unsloth为单机单卡运行，需要指定显卡</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1. 首先进行模型导入</span></span><br><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">max_seq_length = <span class="number">8192</span></span><br><span class="line">dtype = <span class="literal">None</span></span><br><span class="line">load_in_4bit = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = <span class="string">&quot;./Qwen3-8B&quot;</span>,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    dtype = dtype,</span><br><span class="line">    load_in_4bit = load_in_4bit,</span><br><span class="line">)</span><br><span class="line"><span class="comment">#print(model) #查看模型基本情况，包括模型结构和分词器信息等</span></span><br><span class="line"><span class="comment">#print(tokenizer) #查看此时还没有LoRA层。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统信息</span></span><br><span class="line"><span class="comment"># gpu_stats = torch.cuda.get_device_properties(0)</span></span><br><span class="line"><span class="comment"># start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)</span></span><br><span class="line"><span class="comment"># max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)</span></span><br><span class="line"><span class="comment"># print(f&quot;GPU = &#123;gpu_stats.name&#125;. Max memory = &#123;max_memory&#125; GB.&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;&#123;start_gpu_memory&#125; GB of memory reserved.&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果需要Function Calling，需要设置tools</span></span><br><span class="line"><span class="comment"># Function calling能力微调除了系统提示词+思考链+工具并联调用外，还可能出现比如工具串联调用、工具不存在时返回结果、以及多次调用工具无法成功后返回结果等数据，</span></span><br><span class="line"><span class="comment"># 此外，在真实的Function calling能力训练数据集中，还需要包含至少几十种或者上百种API工具调用，才能让大模型本身识别外部工具的能力得到泛化。</span></span><br><span class="line">tools = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;type&quot;</span>: <span class="string">&quot;function&quot;</span>,</span><br><span class="line">        <span class="string">&quot;function&quot;</span>:&#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;get_weather&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;description&#x27;</span>: <span class="string">&#x27;查询即时天气函数，根据输入的城市名称，查询对应城市的实时天气，一次只能输入一个城市名称&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;parameters&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;object&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;properties&#x27;</span>: &#123;</span><br><span class="line">                    <span class="string">&#x27;loc&#x27;</span>: &#123;</span><br><span class="line">                        <span class="string">&#x27;description&#x27;</span>: <span class="string">&quot;城市名称，注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入&#x27;Beijing&#x27;&quot;</span>,</span><br><span class="line">                        <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;string&#x27;</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&#x27;required&#x27;</span>: [<span class="string">&#x27;loc&#x27;</span>]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始对话</span></span><br><span class="line"><span class="comment"># 借助Unsloth进行模型调用总共需要两个步骤，</span></span><br><span class="line"><span class="comment"># 其一是借助apply_chat_template进行分词同时输入对话相关参数，</span></span><br><span class="line"><span class="comment"># 其二则是借助generate进行文本创建。</span></span><br><span class="line"><span class="comment"># 一次基本对话流程如下所示：</span></span><br><span class="line"><span class="comment">#（1）封装</span></span><br><span class="line">messages = [</span><br><span class="line"><span class="comment">#   &#123;&quot;role&quot; : &quot;system&quot;, &quot;content&quot; : &quot;你是一名助人为乐的助手，名叫小明。&quot;&#125;,  #设置系统提示词</span></span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span> : <span class="string">&quot;你好，好久不见！&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = <span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>, </span><br><span class="line">    enable_thinking = <span class="literal">False</span>, <span class="comment"># 设置是否有思考过程，False不思考，True思考</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">#print(text)	#查看加载了Qwen3内置提示词模板之后的字符串</span></span><br><span class="line"><span class="comment">#&lt;|im_start|&gt;user\n你好，好久不见！&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n</span></span><br><span class="line"><span class="comment">#（2）分词</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"><span class="comment">#（3）推理</span></span><br><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,</span><br><span class="line">    attention_mask=inputs.attention_mask,</span><br><span class="line">    max_new_tokens=max_seq_length,</span><br><span class="line">    use_cache=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment">#（4）结果</span></span><br><span class="line"><span class="comment">#print(outputs)</span></span><br><span class="line">response = tokenizer.batch_decode(outputs)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="built_in">print</span>(response[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 外部一个被调用的函数，用于Function Calling，需要在主代码中配置tools</span></span><br><span class="line"><span class="keyword">import</span> requests, json</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weather</span>(<span class="params">loc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    查询即时天气函数</span></span><br><span class="line"><span class="string">    :param loc: 必要参数，字符串类型，用于表示查询天气的具体城市名称，\</span></span><br><span class="line"><span class="string">    注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入&#x27;Beijing&#x27;；</span></span><br><span class="line"><span class="string">    :return：OpenWeather API查询即时天气的结果，具体URL请求地址为：https://api.openweathermap.org/data/2.5/weather\</span></span><br><span class="line"><span class="string">    返回结果对象类型为解析之后的JSON格式对象，并用字符串形式进行表示，其中包含了全部重要的天气信息</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Step 1.构建请求</span></span><br><span class="line">    url = <span class="string">&quot;https://api.openweathermap.org/data/2.5/weather&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2.设置查询参数</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&quot;q&quot;</span>: loc,               </span><br><span class="line">        <span class="string">&quot;appid&quot;</span>: <span class="string">&quot;YOUR_API_KEY&quot;</span>,    <span class="comment"># 输入API key</span></span><br><span class="line">        <span class="string">&quot;units&quot;</span>: <span class="string">&quot;metric&quot;</span>,            <span class="comment"># 使用摄氏度而不是华氏度</span></span><br><span class="line">        <span class="string">&quot;lang&quot;</span>:<span class="string">&quot;zh_cn&quot;</span>                <span class="comment"># 输出语言为简体中文</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3.发送GET请求</span></span><br><span class="line">    response = requests.get(url, params=params)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4.解析响应</span></span><br><span class="line">    data = response.json()</span><br><span class="line">    <span class="keyword">return</span> json.dumps(data)</span><br></pre></td></tr></table></figure>

<p><img src="/../images/llm-finetuning/31.png" alt="截屏2025-08-29 14.31.13"></p>
<h3 id="（2）Unsloth高层对话API"><a href="#（2）Unsloth高层对话API" class="headerlink" title="（2）Unsloth高层对话API"></a>（2）Unsloth高层对话API</h3><p>除了使用上述底层API进行对话外，Unsloth还提供了更加便捷的流式输出模型对话信息的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TextStreamer</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span> : <span class="string">&quot;你好，好久不见！&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = <span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>, </span><br><span class="line">    enable_thinking = <span class="literal">False</span>, 	<span class="comment">#不使用推理</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">_ = model.generate(</span><br><span class="line">    **tokenizer(text, return_tensors = <span class="string">&quot;pt&quot;</span>).to(<span class="string">&quot;cuda&quot;</span>),</span><br><span class="line">    max_new_tokens = <span class="number">256</span>, <span class="comment"># Increase for longer outputs!</span></span><br><span class="line">    temperature = <span class="number">0.7</span>, top_p = <span class="number">0.8</span>, top_k = <span class="number">20</span>, <span class="comment"># For non thinking</span></span><br><span class="line">    streamer = TextStreamer(tokenizer, skip_prompt = <span class="literal">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="2-3-微调流程实践"><a href="#2-3-微调流程实践" class="headerlink" title="2.3 微调流程实践"></a>2.3 微调流程实践</h2><h3 id="（1）LoRA参数注入"><a href="#（1）LoRA参数注入" class="headerlink" title="（1）LoRA参数注入"></a>（1）LoRA参数注入</h3><p>穿插额外的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = <span class="number">32</span>,           <span class="comment"># Choose any number &gt; 0! Suggested 8, 16, 32, 64, 128</span></span><br><span class="line">    target_modules = [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">                      <span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span>,],</span><br><span class="line">    lora_alpha = <span class="number">32</span>,  <span class="comment"># Best to choose alpha = rank or rank*2</span></span><br><span class="line">    lora_dropout = <span class="number">0</span>, <span class="comment"># Supports any, but = 0 is optimized</span></span><br><span class="line">    bias = <span class="string">&quot;none&quot;</span>,    <span class="comment"># Supports any, but = &quot;none&quot; is optimized</span></span><br><span class="line">    <span class="comment"># [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!</span></span><br><span class="line">    use_gradient_checkpointing = <span class="string">&quot;unsloth&quot;</span>, <span class="comment"># True or &quot;unsloth&quot; for very long context</span></span><br><span class="line">    random_state = <span class="number">3407</span>,</span><br><span class="line">    use_rslora = <span class="literal">False</span>,   <span class="comment"># We support rank stabilized LoRA</span></span><br><span class="line">    loftq_config = <span class="literal">None</span>,  <span class="comment"># And LoftQ</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="（2）设置微调参数"><a href="#（2）设置微调参数" class="headerlink" title="（2）设置微调参数"></a>（2）设置微调参数</h3><p>基本训练过程为：</p>
<ol>
<li>从 <code>combined_dataset</code> 中取出一批样本（2 条）</li>
<li>重复上面过程 4 次（<code>gradient_accumulation_steps=4</code>）</li>
<li>将累计的梯度用于更新模型一次参数（等效于一次大 batch 更新）</li>
<li>重复上述过程，直到 <code>max_steps=30</code> 停止</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> SFTTrainer, SFTConfig</span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model = model,</span><br><span class="line">    tokenizer = tokenizer,</span><br><span class="line">    train_dataset = combined_dataset,</span><br><span class="line">    eval_dataset = <span class="literal">None</span>, <span class="comment"># Can set up evaluation!</span></span><br><span class="line">    args = SFTConfig(</span><br><span class="line">        dataset_text_field = <span class="string">&quot;text&quot;</span>,</span><br><span class="line">        per_device_train_batch_size = <span class="number">2</span>,</span><br><span class="line">        gradient_accumulation_steps = <span class="number">4</span>, <span class="comment"># Use GA to mimic batch size!</span></span><br><span class="line">        warmup_steps = <span class="number">5</span>,</span><br><span class="line">        num_train_epochs = <span class="number">1</span>, <span class="comment"># Set this for 1 full training run.</span></span><br><span class="line">        max_steps = <span class="number">30</span>,</span><br><span class="line">        learning_rate = <span class="number">2e-4</span>, <span class="comment"># Reduce to 2e-5 for long training runs</span></span><br><span class="line">        logging_steps = <span class="number">1</span>,</span><br><span class="line">        optim = <span class="string">&quot;adamw_8bit&quot;</span>,</span><br><span class="line">        weight_decay = <span class="number">0.01</span>,</span><br><span class="line">        lr_scheduler_type = <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">        seed = <span class="number">3407</span>,</span><br><span class="line">        report_to = <span class="string">&quot;wandb&quot;</span>, <span class="comment"># Use this for WandB etc</span></span><br><span class="line">    ),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示GPU占用情况</span></span><br><span class="line">used_memory = <span class="built_in">round</span>(torch.cuda.max_memory_reserved() / <span class="number">1024</span> / <span class="number">1024</span> / <span class="number">1024</span>, <span class="number">3</span>)</span><br><span class="line">used_memory_for_lora = <span class="built_in">round</span>(used_memory - start_gpu_memory, <span class="number">3</span>)</span><br><span class="line">used_percentage = <span class="built_in">round</span>(used_memory / max_memory * <span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">lora_percentage = <span class="built_in">round</span>(used_memory_for_lora / max_memory * <span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;trainer_stats.metrics[<span class="string">&#x27;train_runtime&#x27;</span>]&#125;</span> seconds used for training.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">round</span>(trainer_stats.metrics[<span class="string">&#x27;train_runtime&#x27;</span>]/<span class="number">60</span>, <span class="number">2</span>)&#125;</span> minutes used for training.&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Peak reserved memory = <span class="subst">&#123;used_memory&#125;</span> GB.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Peak reserved memory for training = <span class="subst">&#123;used_memory_for_lora&#125;</span> GB.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Peak reserved memory % of max memory = <span class="subst">&#123;used_percentage&#125;</span> %.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Peak reserved memory for training % of max memory = <span class="subst">&#123;lora_percentage&#125;</span> %.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">trainer_stats = trainer.train()</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型保存</span></span><br><span class="line">model.save_pretrained_merged(save_directory = <span class="string">&quot;Qwen3-8B&quot;</span>, </span><br><span class="line">                             tokenizer = tokenizer, </span><br><span class="line">                             save_method = <span class="string">&quot;merged_16bit&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中<code>SFTTrainer</code>：一个专门为指令微调设计的训练器，封装了 Hugging Face 的 <code>Trainer</code>，而<code>SFTConfig</code>：配置训练参数的专用类，功能类似 <code>TrainingArguments</code>。而SFTConfig核心参数解释如下：</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>dataset_text_field=&quot;text&quot;</code></td>
<td>数据集中用于训练的字段名称，如 <code>text</code> 或 <code>prompt</code></td>
</tr>
<tr>
<td><code>per_device_train_batch_size=2</code></td>
<td>每张 GPU 上的 batch size 是 2</td>
</tr>
<tr>
<td><code>gradient_accumulation_steps=4</code></td>
<td>梯度累计 4 次后才进行一次反向传播（等效于总 batch size &#x3D; 2 × 4 &#x3D; 8）</td>
</tr>
<tr>
<td><code>warmup_steps=5</code></td>
<td>前 5 步进行 warmup（缓慢提升学习率）</td>
</tr>
<tr>
<td><code>max_steps=30</code></td>
<td>最多训练 30 步（适合调试或快速实验）</td>
</tr>
<tr>
<td><code>learning_rate=2e-4</code></td>
<td>初始学习率（短训练可用较高值）</td>
</tr>
<tr>
<td><code>logging_steps=1</code></td>
<td>每训练 1 步就打印一次日志</td>
</tr>
<tr>
<td><code>optim=&quot;adamw_8bit&quot;</code></td>
<td>使用 8-bit AdamW 优化器（节省内存，Unsloth 支持）</td>
</tr>
<tr>
<td><code>weight_decay=0.01</code></td>
<td>权重衰减，用于防止过拟合</td>
</tr>
<tr>
<td><code>lr_scheduler_type=&quot;linear&quot;</code></td>
<td>线性学习率调度器（从高到低线性下降）</td>
</tr>
<tr>
<td><code>seed=3407</code></td>
<td>固定随机种子，确保结果可复现</td>
</tr>
<tr>
<td><code>report_to=&quot;none&quot;</code></td>
<td>不使用 WandB 或 TensorBoard 等日志平台（可改为 <code>&quot;wandb&quot;</code>）</td>
</tr>
</tbody></table>
<h3 id="（3）【可选】设置wandb"><a href="#（3）【可选】设置wandb" class="headerlink" title="（3）【可选】设置wandb"></a>（3）【可选】设置wandb</h3><p>接下来可继续设置wandb用于进行模型训练过程关键信息记录。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">os.environ[<span class="string">&quot;WANDB_NOTEBOOK_NAME&quot;</span>] = <span class="string">&quot;Qwen3-8B.ipynb&quot;</span></span><br><span class="line">wandb.login(key=<span class="string">&quot;4b62572b8426ff59ce46fec93a00eb1feecc026a&quot;</span>)</span><br><span class="line">run = wandb.init(project=<span class="string">&#x27;Fine-tune-Qwen-8B&#x27;</span>, )</span><br></pre></td></tr></table></figure>

<h3 id="（4）加载参数测试"><a href="#（4）加载参数测试" class="headerlink" title="（4）加载参数测试"></a>（4）加载参数测试</h3><p>参考上述<strong>章节2.1（8）EvalScope性能测试</strong></p>
<h1 id="3-【补充】数据的格式"><a href="#3-【补充】数据的格式" class="headerlink" title="3. 【补充】数据的格式"></a>3. 【补充】数据的格式</h1><p>目前已经诞生了各类不同的微调框架和海量的微调数据集，在绝大多数情况下，只需要选择不同的微调框架并搭配不同的数据集即可</p>
<p>但随着模型能力越来越复杂，包括现阶段很多模型具备了Function calling功能，或具备了推理或者混合推理能力，此时如果希望进行一些复杂功能模型的微调，例如围绕Qwen3模型进行Function calling能力微调、同时还需保留其混合推理能力，此时很多公开数据集或许就无法满足要求了。</p>
<p>此外，如果我们希望给模型进行特定领域的知识关注，或者提升模型对于特殊工具组的工具调用准确率，此时就需要手动创建微调数据集了。</p>
<h2 id="2-1-数据集格式"><a href="#2-1-数据集格式" class="headerlink" title="2.1 数据集格式"></a>2.1 数据集格式</h2><h3 id="（1）-模型内置特殊字符及提示词模板"><a href="#（1）-模型内置特殊字符及提示词模板" class="headerlink" title="（1） 模型内置特殊字符及提示词模板"></a>（1） 模型内置特殊字符及提示词模板</h3><p>于当代大模型来说，普遍需要通过一些特殊字符来标记用户的不同类型输入、系统提示词、以及工具调用或者多模态输入等。</p>
<p><img src="/../images/llm-finetuning/42.png" alt="截屏2025-08-28 14.42.46"></p>
<p>其中 <code>&lt;|im_start|&gt;</code>代表文本开始，而 <code>user</code>则代表消息身份，用于构建多轮对话，而 <code>&lt;|im_end|&gt;</code>则代表文本结束，即用户输入结束，而 <code>&lt;|im_start|&gt;</code>代表新一段文本开始， <code>assistant</code>代表接下来由模型创建消息，而 <code>&lt;|im_end|&gt;</code>同样代表模型创建消息的结束。</p>
<p>模型其实是通过这样一组特殊字符标记来规范自己的行为，判断当前消息类型，以及通过输出特殊标记来确定停止时间.</p>
<p>对于绝大多数模型，我们可以在模型的 <code>tokenizer_config.json</code>中看到完整的特殊标记符（以及系统提示词模板）：</p>
<p><img src="/../images/llm-finetuning/46.png" alt="截屏2025-08-28 14.46.33"></p>
<p>实际微调过程，需要有监督的数据集、也就是需要输入QA对来进行微调。著名的alpaca_zh中文微调数据集来说，其基本格式如下：</p>
<p><img src="/../images/llm-finetuning/47.png" alt="截屏2025-08-28 14.47.56"></p>
<p>其中的input和output就是输入和输出。对话就可以表示为下列json格式数据集：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;输入：你好。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;输出：你好，有什么可以帮到你的？&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<p>真实的微调过程中，如果是针对Qwen3进行微调，微调脚本会将这条数据集（无论什么格式）转化为如下格式：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;user\n你好&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n你好，有什么可以帮到你的？&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure>

<p>实际训练过程中，模型就会根据assistant前的内容，学习assistant后面的输出内容。</p>
<h3 id="（2）-带有系统提示和Function-calling微调数据集格式"><a href="#（2）-带有系统提示和Function-calling微调数据集格式" class="headerlink" title="（2） 带有系统提示和Function calling微调数据集格式"></a>（2） 带有系统提示和Function calling微调数据集格式</h3><p>一些带有 instruction字段的微调数据集，还是依靠特殊字符带入到微调过程中</p>
<p>例如有一个对话内容和此时模型的输入和输出如下</p>
<ul>
<li><p>系统提示词（instruction）：你是一名助人为乐的助手。</p>
</li>
<li><p>用户输入（input）：你好，好久不见。</p>
</li>
<li><p>助手回复（output）：是的呀，好久不见，最近有什么有趣的事情要和我分享么？</p>
</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是一名助人为乐的助手。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">你好，好久不见。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">是的呀，好久不见，最近有什么有趣的事情要和我分享么？&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure>

<p>即会通过 <code>&lt;|im_start|&gt;system...&lt;|im_end|&gt;</code>来标记系统提示词。实际进行微调时，模型会根据<code>assistant</code>为界，学习<code>assistant</code>之前的文本输入情况下应该如何输出。</p>
<p>如果对话过程中带入了<code>Function calling</code>，此时首先模型会读取提前准备好的<code>tool schema</code>（也可能是自动生成的，例如MCP即可自动创建<code>tool schema</code>）：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tool_schema = <span class="punctuation">[</span><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;get_weather&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;查询指定城市的天气信息&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;parameters&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;object&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;location&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;要查询天气的城市名称&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;required&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;location&quot;</span><span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>

<p>那么对话内容和模型真实的输入和输出内容如下（回复内容就是一条Function call message）：</p>
<ul>
<li><p>系统提示词（instruction）：你是一名助人为乐的助手。当用户查询天气的时候，请调用get_weather函数进行天气信息查询。</p>
</li>
<li><p>用户输入（input）：你好，请帮我查询下北京天气。</p>
</li>
<li><p>助手回复（output）：{“name”: “get_weather”, “arguments”: {“location”: “北京”}}</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是一名助人为乐的助手。当用户查询天气的时候，请调用get_weather函数进行天气信息查询。</span><br><span class="line"># Tools</span><br><span class="line">You may call one or more functions to assist with the user query.</span><br><span class="line">You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:</span><br><span class="line">&lt;tools&gt;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;get_weather&quot;,</span><br><span class="line">  &quot;description&quot;: &quot;查询指定城市的天气信息&quot;,</span><br><span class="line">  &quot;parameters&quot;: &#123;</span><br><span class="line">    &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">    &quot;properties&quot;: &#123;</span><br><span class="line">      &quot;location&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;要查询天气的城市名称&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;required&quot;: [&quot;location&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&lt;/tools&gt;</span><br><span class="line">For each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:</span><br><span class="line">&lt;tool_call&gt;</span><br><span class="line">  &#123;&quot;name&quot;: &lt;function-name&gt;,&quot;arguments&quot;: &lt;args-json-object&gt;&#125;</span><br><span class="line">&lt;/tool_call&gt;</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line"></span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">你好，请帮我查询下北京天气。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&lt;tool_call&gt;</span><br><span class="line">  &#123;&quot;name&quot;: &quot;get_weather&quot;,&quot;arguments&quot;: &#123;&quot;location&quot;: &quot;北京&quot;&#125;&#125;</span><br><span class="line">&lt;/tool_call&gt;&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure>

<p>训练时，模型同样根据assistant前的内容，学习assistant后面的输出内容。</p>
<p>注意:由于高效微调调整的参数量较少，因此只能优化模型的Function calling能力，并不能从无到有让模型学会Function calling</p>
<h3 id="（3）-带有思考链的微调数据集结构"><a href="#（3）-带有思考链的微调数据集结构" class="headerlink" title="（3） 带有思考链的微调数据集结构"></a>（3） 带有思考链的微调数据集结构</h3><p>带有思考链，则一个简单的问答数据如下</p>
<p><img src="/../images/llm-finetuning/04.png" alt="截屏2025-08-28 15.04.34"></p>
<ul>
<li><p>系统提示词（instruction）：你是一名助人为乐的助手。</p>
</li>
<li><p>用户输入（input）：你好，好久不见。</p>
</li>
<li><p>助手回复（output）：好的，用户发来“你好，好久不见！”，我需要回应。首先，用户可能希望得到亲切的回应，所以应该用友好的语气。&#x2F;n是的呀，好久不见，最近有什么有趣的事情要和我分享么？</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是一名助人为乐的助手。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">你好，好久不见。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&lt;think&gt;</span><br><span class="line">好的，用户发来“你好，好久不见！，我需要回应。首先，用户可能希望得到亲切的回应，所以应该用友好的语气。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">是的呀，好久不见，最近有什么有趣的事情要和我分享么？&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure>

<p>模型同样根据assistant前的内容，学习assistant后面的输出内容</p>
<h3 id="（4）带有思考过程、系统提示词的Function-calling"><a href="#（4）带有思考过程、系统提示词的Function-calling" class="headerlink" title="（4）带有思考过程、系统提示词的Function calling"></a>（4）带有思考过程、系统提示词的Function calling</h3><p><img src="/../images/llm-finetuning/07.png" alt="截屏2025-08-28 15.07.11"></p>
<ul>
<li><p>系统提示词（instruction）：你是一名助人为乐的助手。当用户查询天气的时候，请调用get_weather函数进行天气信息查询。</p>
</li>
<li><p>用户输入（input）：你好，请帮我查询下北京天气。</p>
</li>
<li><p>助手回复（output）：好的，用户问北京今天的天气，我应该尝试调用工具get_weather，并将参数设置为北京。&#x2F;n{“name”: “get_weather”, “arguments”: {“location”: “北京”}}</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是一名助人为乐的助手。当用户查询天气的时候，请调用get_weather函数进行天气信息查询。</span><br><span class="line"># Tools</span><br><span class="line">You may call one or more functions to assist with the user query.</span><br><span class="line">You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:</span><br><span class="line">&lt;tools&gt;</span><br><span class="line">&#123;&quot;name&quot;: &quot;get_weather&quot;</span><br><span class="line">&#123;&quot;type&quot;: &quot;object&quot;,</span><br><span class="line">&quot;要查询天气的城市名称&quot;&#125;&#125;,</span><br><span class="line">&lt;/tools&gt;,</span><br><span class="line">&quot;description&quot;: &quot;查询指定城市的天气信息&quot;,</span><br><span class="line">&quot;properties&quot;: &#123;&quot;location&quot;: &#123;&quot;type&quot;: &quot;string&quot;</span><br><span class="line">&quot;required&quot;: [&quot;location&quot;]&#125;&#125;</span><br><span class="line">&quot;parameters&quot;:,</span><br><span class="line">&quot;description&quot;:</span><br><span class="line">For each function call, return a json object with function name and arguments</span><br><span class="line">within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:</span><br><span class="line">&lt;tool_call&gt;</span><br><span class="line">	&#123;&quot;name&quot;: &lt;function-name&gt;,</span><br><span class="line">	&quot;arguments&quot;: &lt;args-json-object&gt;&#125;</span><br><span class="line">&lt;/tool_call&gt;</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line"></span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">你好，请帮我查询下北京天气。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&lt;think&gt;</span><br><span class="line">好的，用户问北京今天的天气，我应该尝试调用工具 get_weather，并将参数设置为北京。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line">&lt;tool_call&gt;</span><br></pre></td></tr></table></figure>

<p>同样根据assistant前的内容，学习assistant后面的输出内容</p>
<h3 id="（5）混合推理模型构造微调数据集"><a href="#（5）混合推理模型构造微调数据集" class="headerlink" title="（5）混合推理模型构造微调数据集"></a>（5）混合推理模型构造微调数据集</h3><p>构造微调数据集，一般来说可以在huggingface、ModelScope或llama-factory中挑选合适的数据集，并根据实际情况进行组</p>
<p>装。</p>
<p>例如围绕Qwen3模型的高效微调，为了确保其仍然保留混合推理能力，我们可以考虑在微调数据集中加入如普通对话数据集FineTome，以及带有推理字段的数学类数据集OpenMathReasoning，并围绕这<strong>两个数据集进行拼接</strong>，从而在确保能提升模型的数学能力的同时，保留非推理的功能。同时还需要在持续微调训练过程中不断调整COT数学数据集和普通文本问答数据集<strong>之间的配比</strong>，以确保模型能够在提升数学能力的同时，保留混合推理的性能。</p>
<h3 id="（6）其他"><a href="#（6）其他" class="headerlink" title="（6）其他"></a>（6）其他</h3><ol>
<li>Alpaca（单轮指令微调）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;instruction&quot;: &quot;请把下面中文翻译成英文&quot;,</span><br><span class="line">  &quot;input&quot;: &quot;大模型数据集格式&quot;,</span><br><span class="line">  &quot;output&quot;: &quot;Dataset format for large models&quot;,</span><br><span class="line">  &quot;system&quot;: &quot;你是一个翻译助手&quot;   // 可选</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>若有多轮历史，加 <code>&quot;history&quot;: [[&quot;上一轮用户问题&quot;,&quot;上一轮模型回答&quot;], …]</code></p>
<ol start="2">
<li>ShareGPT（多轮对话，角色更丰富）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;conversations&quot;:[</span><br><span class="line">    &#123;&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;你好&quot;&#125;,</span><br><span class="line">    &#123;&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;你好，请问有什么可以帮您的？&quot;&#125;,</span><br><span class="line">    &#123;&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;介绍下北京&quot;&#125;,</span><br><span class="line">    &#123;&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;北京是中华人民共和国首都……&quot;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>纯问答 jsonl（预训练或继续预训练）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;text&quot;: &quot;大模型数据集格式指的是……&quot;&#125;</span><br><span class="line">&#123;&quot;text&quot;: &quot;下一篇文档内容……&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>只有一行纯文本，无监督信号，常用于增量预训练。</p>
<ol start="4">
<li>偏好对齐&#x2F;强化学习格式，DPO &#x2F; PPO &#x2F; RLHF</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;prompt&quot;: &quot;请用三句话介绍人工智能&quot;,</span><br><span class="line">  &quot;chosen&quot;: &quot;人工智能是……&quot;,</span><br><span class="line">  &quot;rejected&quot;: &quot;人工智能啊，就是……&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>三列必须同时出现，分别表示“提示、优质回答、劣质回答</p>

        </div>

      <div style="background-color: #f9f9f9; border-left: 5px solid #007acc; padding: 10px; margin: 20px 0;">
        <strong>📢 转载须知：</strong>本站点内容采用
    <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 协议</a>进行许可，转载请注明原文链接和作者信息
      </div>


        <!--  -->
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"># 大模型</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span>· </span>
                <a href="/">主页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2025/08/28/%E5%9C%86%E5%9C%86%E7%9A%84%E8%82%A5%E7%9A%82%E6%B3%A1/">圆圆的肥皂泡</a>
            
            
            <a class="next" rel="next" href="/2025/08/26/Ollama%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/">Ollama大模型部署</a>
            
        </section>


    </article>
</div>

            </div>
            <!-- <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Henry Li | Powered by Hexo</a> & Chic</a></span>
    </div>
</footer> -->
<!-- <footer id="footer" class="footer">
  <div class="copyright">
    <span>© 2025 Henry Li | Powered by Hexo</a> & Chic</a></span>
    <br>
    <span id="busuanzi_container_site_uv">本站总访客数: <span id="busuanzi_value_site_uv"></span> 人,</span>
    <span id="busuanzi_container_site_pv">本站总访问量: <span id="busuanzi_value_site_pv"></span> 次</span>
    <br>
    <span> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">
      <img alt="CC BY-NC 4.0" style="position:relative; top:2px;" src="https://licensebuttons.net/l/by-nc/4.0/80x15.png" />
      </a>除特别声明外，本站点内容采用<a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
    CC BY-NC-SA 4.0 协议</a>进行许可，转载请注明出处。
<span>
  </div>

  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</footer> -->
<footer id="footer" class="footer" style="font-size:0.3em;line-height:1.5">
  <br>
  <div class="copyright">
    <span>© 2025 Henry Li | Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank" rel="noopener">Chic</a></span>
    <br>
    <span id="busuanzi_container_site_uv">本站总访客数: <span id="busuanzi_value_site_uv"></span> 人,</span>
    <span id="busuanzi_container_site_pv">本站总访问量: <span id="busuanzi_value_site_pv"></span> 次</span>
    <br>
    <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
      <img alt="CC BY-NC-SA 4.0" style="position:relative; top:3px;" src="https://licensebuttons.net/l/by-nc-sa/4.0/80x15.png" />
    </a>本站点内容采用
    <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
      CC BY-NC-SA 4.0 版权协议</a>进行许可，转载请注明原文链接和作者信息
  </div>

  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</footer>
    </div>
</body>

</html>